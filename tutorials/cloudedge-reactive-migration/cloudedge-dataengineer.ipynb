{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acdbd28-a67a-41b4-b4e6-16e3a0a5626e",
   "metadata": {},
   "source": [
    "# CloudEdge DataEngineer (Inference Stage)\n",
    "\n",
    "****Inference Scenarios****\n",
    "\n",
    "| scenarios | reference app | framework | model/dataset |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| batch-inference-workflow | [scenarios/job-pipeline](https://github.com/peiniliu/inference/tree/dev/vision/classification_and_detection/scenarios/job-pipeline) | tensorflow | resnet/dumy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95d7f3-b42a-4c13-8902-e3ca6f2de632",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Make sure to set these environment variables in your session with the proper values. All of them are mandatory except:\n",
    "- `DOCKER_REGISTRY`: if you plan to push the images to a private registry\n",
    "- `DOCKER_TAG`: if you don't want to leave the default `latest` tag\n",
    "- `DOCKER_REGISTRY_USERNAME`: if your private registry requires authentication\n",
    "- `DOCKER_REGISTRY_PASSWORD`: if your private registry requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f31ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for debug purposes, don't leave them enable in the repository!!!\n",
    "%env WORKDIR=/root/cloudskin/data-connector\n",
    "%env REACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-reactive-migration/dataengineer\n",
    "%env SCANFLOW_SERVER_URI=http://10.0.26.8:32002\n",
    "%env SCANFLOW_TRACKER_URI=http://10.0.26.8:32002\n",
    "%env MLFLOW_S3_ENDPOINT_URL=http://10.0.26.8:32000\n",
    "# MinIO API endpoint, not console!\n",
    "%env AWS_ACCESS_KEY_ID=admin\n",
    "%env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "%env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "%env DOCKER_TAG=feat/reactive-migration\n",
    "%env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "%env DOCKER_REGISTRY_PASSWORD=sEPN9vEtSjCFodUvVa1M\n",
    "# This is to avoid CI pipelines to deploy anything\n",
    "%env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca50738-a9da-45ec-97fa-9d1722b71dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,'../..')\n",
    "\n",
    "from scanflow.client import ScanflowClient\n",
    "from scanflow.client import ScanflowTrackerClient\n",
    "from scanflow.client import ScanflowDeployerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed2df1-257b-4bc5-a641-2d65a168dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanflow.tools import env\n",
    "print(env.get_env(\"SCANFLOW_SERVER_URI\"))\n",
    "print(env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "print(env.get_env(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "print(env.get_env(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(env.get_env(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "print(env.get_env(\"DOCKER_REGISTRY\"))\n",
    "print(env.get_env(\"DOCKER_TAG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeae4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App folder - Must point to the folder includeing all 'dataengineer' and 'datascience' folders\n",
    "# for cloudedge-reactive-migration, allocated in examples/cloudedge-reactive-migration\n",
    "app_dir = os.path.join(env.get_env('WORKDIR'), env.get_env('REACTIVE_MIGRATION_DATAENGINEER_APP_DIR'))\n",
    "print(app_dir)\n",
    "app_name = \"cloudedge-reactive-migration\"\n",
    "team_name = \"dataengineer\"\n",
    "\n",
    "# Initialize the Scanflow Client\n",
    "client = ScanflowClient(\n",
    "    #if you defined \"SCANFLOW_SERVER_URI\", you dont need to provide this\n",
    "    registry=env.get_env(\"DOCKER_REGISTRY\"),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddf4c1",
   "metadata": {},
   "source": [
    "## Batch-inference-graph for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a7fd2",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfbbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor stages\n",
    "# - Executor 1: Data retrieval from Prometheus\n",
    "# - Executor 2: Data pre-processing + QoS Predictor\n",
    "executor_1 = client.ScanflowExecutor(\n",
    "    name=\"data-retrieval\",\n",
    "    mainfile=\"data-retrieval.py\",\n",
    "    dockerfile=\"Dockerfile_data_retrieval_no_buildkit\",\n",
    "    parameters={\n",
    "        'app_name': app_name,\n",
    "        'team_name': team_name,\n",
    "        #'promcsv_config': \"/app/data-retrieval/promql_queries.json\" # Config file already included in the Docker image\n",
    "        'promcsv_config': \"/workflow/promql_queries.json\" # Config file for debug purposes, manually included in the workflow PVC\n",
    "    }\n",
    ")\n",
    "\n",
    "executor_2 = client.ScanflowExecutor(\n",
    "    name=\"qos-upload\",\n",
    "    mainfile=\"qos-upload.py\",\n",
    "    dockerfile=\"Dockerfile_qos_upload_no_buildkit\",\n",
    "    parameters={\n",
    "        'name': \"QoS preprocessing and upload\",\n",
    "        'app_name': app_name,\n",
    "        'team_name': team_name,\n",
    "        'csv_path': \"/workflow/migration_experiment\", # We expect each experiment run to store results at /workflow/migration_experiment/run_at_${execution_timestamp} folder\n",
    "        'csv_sep': \";\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Stages dependencies\n",
    "# TODO: define them once other stages have been developed\n",
    "dependency_1 = client.ScanflowDependency(\n",
    "    dependee='data-retrieval',\n",
    "    depender='qos-upload'\n",
    ")\n",
    "\n",
    "# Predictor workflow: batch-inference-reactive-graph\n",
    "# TODO: add missing executors and dependencies\n",
    "workflow_1 = client.ScanflowWorkflow(\n",
    "    name=\"batch-inference-reactive-graph\",\n",
    "    nodes=[executor_1, executor_2],\n",
    "    edges=[dependency_1],\n",
    "    type=\"batch\",\n",
    "    cron=\"*/5 * * * *\",\n",
    "    output_dir=\"/workflow\",\n",
    "    image_pull_secrets=[\"cloudskin-registry\"] # Required for Workflow templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1117d",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077d38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = client.ScanflowAgentSensor_IntervalTrigger(minutes=5)\n",
    "sensor = client.ScanflowAgentSensor(\n",
    "    name=\"reactive_watch_qos\",\n",
    "    isCustom=True,\n",
    "    func_name=\"reactive_watch_qos\",\n",
    "    trigger=trigger,\n",
    "    kwargs={\n",
    "        'frequency': 300\n",
    "    }\n",
    ")\n",
    "planner = client.ScanflowAgent(\n",
    "    name=\"planner\",\n",
    "    dockerfile=\"Dockerfile_scanflow_planner\",\n",
    "    template=\"planner\",\n",
    "    sensors=[sensor],\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes cluster (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83129",
   "metadata": {},
   "source": [
    "### Compose the Scanflow Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fd7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = client.ScanflowApplication(\n",
    "    app_name=app_name,\n",
    "    app_dir=app_dir,\n",
    "    team_name=team_name,\n",
    "    workflows=[workflow_1],\n",
    "    agents=[planner]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ed3b3",
   "metadata": {},
   "source": [
    "### DEBUG: show application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6133549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339ddc6",
   "metadata": {},
   "source": [
    "### Build the Scanflow Application\n",
    "- This step builds the Docker images for all the Scanflow executors and uploads them to the container registry (currently hardcoded in the `scanflow` module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Scanflow Tracker Port (32766)\n",
    "build_app = client.build_ScanflowApplication(\n",
    "    app=app,\n",
    "    trackerPort=32766,\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d71d4",
   "metadata": {},
   "source": [
    "### DEBUG: show built application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6d14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de650749",
   "metadata": {},
   "source": [
    "### Create a ScanflowDeployerClient\n",
    "\n",
    "This client creates the required environment for Scanflow to run the pipelines in a Kubernetes cluster based on the built application. It can:\n",
    "\n",
    "- Create an environment for the Scanflow application within its own namespace\n",
    "- Deploy a local Scanflow Tracker\n",
    "- Run the application as an Argo Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd58a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the deployer client\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    deployer_client = ScanflowDeployerClient(\n",
    "        user_type=\"local\",\n",
    "        deployer=\"argo\",\n",
    "        k8s_config_file=\"/root/.kube/config\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51a4e2",
   "metadata": {},
   "source": [
    "### Deploy the ScanflowEnvironment\n",
    "This creates:\n",
    "- A namespace for the application\n",
    "- A Deployment for the local scanflow tracker\n",
    "- A Deployment for all the agents (in this case there's only the planner)\n",
    "  - Planner doesn't include right now the `scanflow` module, so it must be copied inside the planner's PVC so the container finds it in the `/scanflow/scanflow/scanflow` path\n",
    "\n",
    "Go to your Kubernetes cluster and check that both tracker and planner pods are Running without errors in the `scanflow-cloudedge-reactive-migration-dataengineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbb638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose a custom ScanflowEnvironment\n",
    "from scanflow.deployer.env import ScanflowEnvironment\n",
    "data_eng_env = ScanflowEnvironment()\n",
    "data_eng_env.namespace=f\"scanflow-{build_app.app_name}-{build_app.team_name}\"\n",
    "# TRACKER STORAGE MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "# - \"scanflow\" db must already exist in postgresql\n",
    "# - \"scanflow\" bucket must already exist in MinIO \n",
    "data_eng_env.tracker_config.TRACKER_STORAGE = f\"postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\"\n",
    "data_eng_env.tracker_config.TRACKER_ARTIFACT = f\"s3://scanflow/{data_eng_env.namespace}\"\n",
    "# CLIENT CONFIG: REPLACE WITH CURRENTLY DEPLOYED SERVICES IN \"scanflow-server\" namespace\n",
    "#data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = f\"http://scanflow-tracker.{data_eng_env.namespace}\"\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "data_eng_env.client_config.SCANFLOW_SERVER_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "# MINIO MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "data_eng_env.secret.AWS_ACCESS_KEY_ID = \"admin\"\n",
    "data_eng_env.secret.AWS_SECRET_ACCESS_KEY = \"scanflow123\"\n",
    "data_eng_env.secret.MLFLOW_S3_ENDPOINT_URL = \"http://minio.scanflow-server:9000\"\n",
    "data_eng_env.secret.AWS_ENDPOINT_URL = \"http://minio.scanflow-server:9000\"\n",
    "# NEW: configure image pull secret\n",
    "data_eng_env.image_pull_secret.name = \"cloudskin-registry\"\n",
    "data_eng_env.image_pull_secret.registry = env.get_env(\"DOCKER_REGISTRY\")\n",
    "data_eng_env.image_pull_secret.username = env.get_env(\"DOCKER_REGISTRY_USERNAME\")\n",
    "data_eng_env.image_pull_secret.password = env.get_env(\"DOCKER_REGISTRY_PASSWORD\")\n",
    "data_eng_env.image_pull_secret.email = \"cloudskin-project@bsc.es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba826687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application environment\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.create_environment(\n",
    "        app=build_app,\n",
    "        scanflowEnv=data_eng_env\n",
    "    )\n",
    "\n",
    "    # TODO: Retrieve the PV name of the PVC and copy `scanflow` module there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59425f90",
   "metadata": {},
   "source": [
    "## Run Workflow to test\n",
    "This composes an Argo CronWorkflow for the application and submits it to the Argo Workflows engine:\n",
    "- Pre-requisites: Argo Workflows must be set to use the `default` service account when no `serviceAccount` is provided in the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677efde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.run_app(app=build_app)\n",
    "    # DEBUG - TODO: if using external config files, automate their copy inside the workflow PVC instead of doing it manually\n",
    "    # - Copy Promcsv config file so it is available within the container in the /workflow/promql_queries.json path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2fe2e",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306527d",
   "metadata": {},
   "source": [
    "### Remove Scanflow application\n",
    "This will delete the target Scanflow application:\n",
    "- Remove its Argo Workflow object\n",
    "- Remove its PVC and related PV (created during Argo Workflow execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05912d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#await deployer_client.delete_app(app=build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f087cdd",
   "metadata": {},
   "source": [
    "### Remove Scanflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#await deployer_client.clean_environment(app=build_app, scanflow_env=data_eng_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
