{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acdbd28-a67a-41b4-b4e6-16e3a0a5626e",
   "metadata": {},
   "source": [
    "# CloudEdge DataEngineer (Inference Stage)\n",
    "\n",
    "****Inference Scenarios****\n",
    "\n",
    "| scenarios | reference app | framework | model/dataset |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| batch-inference-workflow | [scenarios/job-pipeline](https://github.com/peiniliu/inference/tree/dev/vision/classification_and_detection/scenarios/job-pipeline) | tensorflow | resnet/dumy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95d7f3-b42a-4c13-8902-e3ca6f2de632",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Make sure to set these environment variables in your session with the proper values. All of them are mandatory except:\n",
    "- `DOCKER_REGISTRY`: if you plan to push the images to a private registry\n",
    "- `DOCKER_TAG`: if you don't want to leave the default `latest` tag\n",
    "- `DOCKER_REGISTRY_USERNAME`: if your private registry requires authentication\n",
    "- `DOCKER_REGISTRY_PASSWORD`: if your private registry requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f31ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only for debug purposes, don't leave them enable in the repository!!!\n",
    "# %env WORKDIR=/root/cloudskin/data-connector\n",
    "# %env KUBECONFIG_PATH=/root/.kube/config\n",
    "# %env REACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-reactive-migration/dataengineer\n",
    "# %env SCANFLOW_SERVER_URI=http://10.0.26.8:32002\n",
    "# %env SCANFLOW_TRACKER_URI=http://10.0.26.8:32002\n",
    "# %env MLFLOW_S3_ENDPOINT_URL=http://10.0.26.8:32000\n",
    "# # PostgreSQL URI with credentials\n",
    "# %env SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\n",
    "# # MinIO API endpoint, not console!\n",
    "# %env AWS_ACCESS_KEY_ID=admin\n",
    "# %env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "# %env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# # If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "# %env DOCKER_TAG=feat/reactive-migration\n",
    "# %env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "# %env DOCKER_REGISTRY_PASSWORD=fake-password\n",
    "# %env SCANFLOW_APP_NAME=cloudskin-migration-experiment\n",
    "# %env SCANFLOW_TEAM_NAME=dataengineer\n",
    "# # This is to avoid CI pipelines to deploy anything\n",
    "# %env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79daff1",
   "metadata": {},
   "source": [
    "## Pre-run cleanup\n",
    "\n",
    "Make sure that the experiment isn't already running by removing its namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Make sure \"scanflow\" path is added in available module paths\n",
    "sys.path.insert(0,'../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as cli\n",
    "from kubernetes import config as conf\n",
    "from scanflow.tools import env\n",
    "\n",
    "conf.load_kube_config(config_file=env.get_env(\"KUBECONFIG_PATH\"))\n",
    "client = cli.CoreV1Api()\n",
    "\n",
    "namespaces = client.list_namespace()\n",
    "# Compose the expected namespace that Scanflow creates based on app_name and team_name\n",
    "environment_namespace = f\"scanflow-{env.get_env('SCANFLOW_APP_NAME')}-{env.get_env('SCANFLOW_TEAM_NAME')}\"\n",
    "\n",
    "# Remove the namespace if it exists\n",
    "for namespace in namespaces.items:\n",
    "    if environment_namespace == namespace.metadata.name:\n",
    "        print(f\"{environment_namespace} found! Deleting it...\")\n",
    "        client.delete_namespace(environment_namespace)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ad0a5",
   "metadata": {},
   "source": [
    "## ScanflowClient initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca50738-a9da-45ec-97fa-9d1722b71dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanflow.client import ScanflowClient\n",
    "from scanflow.client import ScanflowDeployerClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208ebc0",
   "metadata": {},
   "source": [
    "### Debug: available environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed2df1-257b-4bc5-a641-2d65a168dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.get_env(\"SCANFLOW_SERVER_URI\"))\n",
    "print(env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "print(env.get_env(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "print(env.get_env(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(env.get_env(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "print(env.get_env(\"DOCKER_REGISTRY\"))\n",
    "print(env.get_env(\"DOCKER_TAG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeae4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App folder - Must point to the folder includeing all 'dataengineer' and 'datascience' folders\n",
    "# for cloudedge-reactive-migration, allocated in examples/cloudedge-reactive-migration\n",
    "app_dir = os.path.join(env.get_env('WORKDIR'), env.get_env('REACTIVE_MIGRATION_DATAENGINEER_APP_DIR'))\n",
    "print(app_dir)\n",
    "app_name = env.get_env(\"SCANFLOW_APP_NAME\")\n",
    "team_name = env.get_env(\"SCANFLOW_TEAM_NAME\")\n",
    "\n",
    "# Initialize the Scanflow Client\n",
    "client = ScanflowClient(\n",
    "    #if you defined \"SCANFLOW_SERVER_URI\", you dont need to provide this\n",
    "    registry=env.get_env(\"DOCKER_REGISTRY\"),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddf4c1",
   "metadata": {},
   "source": [
    "## Batch-inference-graph for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a7fd2",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfbbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor stages\n",
    "# - Executor 1: Data retrieval from Prometheus\n",
    "# - Executor 2: Data pre-processing + QoS Predictor\n",
    "executor_1 = client.ScanflowExecutor(\n",
    "    name=\"data-retrieval\",\n",
    "    mainfile=\"data-retrieval.py\",\n",
    "    dockerfile=\"Dockerfile_data_retrieval_no_buildkit\",\n",
    "    parameters={\n",
    "        'app_name': app_name,\n",
    "        'team_name': team_name,\n",
    "        #'promcsv_config': \"/app/data-retrieval/promql_queries.json\" # Config file already included in the Docker image\n",
    "        'promcsv_config': \"/workflow/promql_queries.json\" # Config file for debug purposes, manually included in the workflow PVC\n",
    "    }\n",
    ")\n",
    "\n",
    "executor_2 = client.ScanflowExecutor(\n",
    "    name=\"qos-upload\",\n",
    "    mainfile=\"qos-upload.py\",\n",
    "    dockerfile=\"Dockerfile_qos_upload_no_buildkit\",\n",
    "    parameters={\n",
    "        'name': \"QoS preprocessing and upload\",\n",
    "        'app_name': app_name,\n",
    "        'team_name': team_name,\n",
    "        'csv_path': \"/workflow/migration_experiment\", # We expect each experiment run to store results at /workflow/migration_experiment/run_at_${execution_timestamp} folder\n",
    "        'csv_sep': \";\",\n",
    "        'purge_local_results': True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Stages dependencies\n",
    "# TODO: define them once other stages have been developed\n",
    "dependency_1 = client.ScanflowDependency(\n",
    "    dependee='data-retrieval',\n",
    "    depender='qos-upload'\n",
    ")\n",
    "\n",
    "# Predictor workflow: batch-inference-reactive-graph\n",
    "# TODO: add missing executors and dependencies\n",
    "workflow_1 = client.ScanflowWorkflow(\n",
    "    name=\"batch-inference-reactive-graph\",\n",
    "    nodes=[executor_1, executor_2],\n",
    "    edges=[dependency_1],\n",
    "    type=\"batch\",\n",
    "    cron=\"*/5 * * * *\",\n",
    "    output_dir=\"/workflow\",\n",
    "    image_pull_secrets=[\"cloudskin-registry\"] # Required for Workflow templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1117d",
   "metadata": {},
   "source": [
    "### Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077d38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = client.ScanflowAgentSensor_IntervalTrigger(minutes=5)\n",
    "sensor = client.ScanflowAgentSensor(\n",
    "    name=\"reactive_watch_qos\",\n",
    "    isCustom=True,\n",
    "    func_name=\"reactive_watch_qos\",\n",
    "    trigger=trigger,\n",
    "    kwargs={\n",
    "        'frequency': 300\n",
    "    }\n",
    ")\n",
    "planner = client.ScanflowAgent(\n",
    "    name=\"planner\",\n",
    "    dockerfile=\"Dockerfile_scanflow_planner\",\n",
    "    template=\"planner\",\n",
    "    sensors=[sensor],\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes cluster (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83129",
   "metadata": {},
   "source": [
    "### Compose the Scanflow Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fd7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = client.ScanflowApplication(\n",
    "    app_name=app_name,\n",
    "    app_dir=app_dir,\n",
    "    team_name=team_name,\n",
    "    workflows=[workflow_1],\n",
    "    agents=[planner]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ed3b3",
   "metadata": {},
   "source": [
    "### DEBUG: show application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6133549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339ddc6",
   "metadata": {},
   "source": [
    "### Build the Scanflow Application\n",
    "- This step builds the Docker images for all the Scanflow executors and uploads them to the container registry (currently hardcoded in the `scanflow` module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Scanflow Tracker Port (32766)\n",
    "build_app = client.build_ScanflowApplication(\n",
    "    app=app,\n",
    "    trackerPort=32766,\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d71d4",
   "metadata": {},
   "source": [
    "### DEBUG: show built application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6d14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de650749",
   "metadata": {},
   "source": [
    "### Create a ScanflowDeployerClient\n",
    "\n",
    "This client creates the required environment for Scanflow to run the pipelines in a Kubernetes cluster based on the built application. It can:\n",
    "\n",
    "- Create an environment for the Scanflow application within its own namespace\n",
    "- Deploy a local Scanflow Tracker\n",
    "- Run the application as an Argo Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd58a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the deployer client\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    deployer_client = ScanflowDeployerClient(\n",
    "        user_type=\"local\",\n",
    "        deployer=\"argo\",\n",
    "        k8s_config_file=env.get_env(\"KUBECONFIG_PATH\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51a4e2",
   "metadata": {},
   "source": [
    "### Deploy the ScanflowEnvironment\n",
    "This creates:\n",
    "- A namespace for the application\n",
    "- A Deployment for the local scanflow tracker\n",
    "- A Deployment for all the agents (in this case there's only the planner)\n",
    "  - Planner doesn't include right now the `scanflow` module, so it must be copied inside the planner's PVC so the container finds it in the `/scanflow/scanflow/scanflow` path\n",
    "\n",
    "Go to your Kubernetes cluster and check that both tracker and planner pods are Running without errors in the `scanflow-cloudedge-reactive-migration-dataengineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbb638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose a custom ScanflowEnvironment\n",
    "from scanflow.deployer.env import ScanflowEnvironment\n",
    "data_eng_env = ScanflowEnvironment()\n",
    "data_eng_env.namespace=f\"scanflow-{build_app.app_name}-{build_app.team_name}\"\n",
    "# TRACKER STORAGE MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "# - \"scanflow\" db must already exist in postgresql\n",
    "# - \"scanflow\" bucket must already exist in MinIO\n",
    "#data_eng_env.tracker_config.TRACKER_STORAGE = f\"postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\"\n",
    "data_eng_env.tracker_config.TRACKER_STORAGE = env.get_env(\"SCANFLOW_TRACKER_STORAGE\")\n",
    "data_eng_env.tracker_config.TRACKER_ARTIFACT = f\"s3://scanflow/{data_eng_env.namespace}\"\n",
    "# CLIENT CONFIG: REPLACE WITH CURRENTLY DEPLOYED SERVICES IN \"scanflow-server\" namespace\n",
    "#data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "#data_eng_env.client_config.SCANFLOW_TRACKER_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "#data_eng_env.client_config.SCANFLOW_SERVER_URI = f\"http://scanflow-server-tracker-service.scanflow-server\"\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = env.get_env(\"SCANFLOW_SERVER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_URI = env.get_env(\"SCANFLOW_SERVER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_SERVER_URI = env.get_env(\"SCANFLOW_SERVER_URI\")\n",
    "# MINIO MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "data_eng_env.secret.AWS_ACCESS_KEY_ID = env.get_env(\"AWS_ACCESS_KEY_ID\")\n",
    "data_eng_env.secret.AWS_SECRET_ACCESS_KEY = env.get_env(\"AWS_SECRET_ACCESS_KEY\")\n",
    "data_eng_env.secret.MLFLOW_S3_ENDPOINT_URL = env.get_env(\"MLFLOW_S3_ENDPOINT_URL\")\n",
    "data_eng_env.secret.AWS_ENDPOINT_URL = env.get_env(\"AWS_ENDPOINT_URL\")\n",
    "# NEW: configure image pull secret\n",
    "data_eng_env.image_pull_secret.name = \"cloudskin-registry\"\n",
    "data_eng_env.image_pull_secret.registry = env.get_env(\"DOCKER_REGISTRY\")\n",
    "data_eng_env.image_pull_secret.username = env.get_env(\"DOCKER_REGISTRY_USERNAME\")\n",
    "data_eng_env.image_pull_secret.password = env.get_env(\"DOCKER_REGISTRY_PASSWORD\")\n",
    "data_eng_env.image_pull_secret.email = \"cloudskin-project@bsc.es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba826687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application environment\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.create_environment(\n",
    "        app=build_app,\n",
    "        scanflowEnv=data_eng_env\n",
    "    )\n",
    "\n",
    "    # TODO: Retrieve the PV name of the PVC and copy `scanflow` module there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59425f90",
   "metadata": {},
   "source": [
    "## Run Workflow to test\n",
    "This composes an Argo CronWorkflow for the application and submits it to the Argo Workflows engine:\n",
    "- Pre-requisites: Argo Workflows must be set to use the `default` service account when no `serviceAccount` is provided in the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677efde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.run_app(app=build_app)\n",
    "    # DEBUG - TODO: if using external config files, automate their copy inside the workflow PVC instead of doing it manually\n",
    "    # - Copy Promcsv config file so it is available within the container in the /workflow/promql_queries.json path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2fe2e",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306527d",
   "metadata": {},
   "source": [
    "### Remove Scanflow application\n",
    "This will delete the target Scanflow application:\n",
    "- Remove its Argo Workflow object\n",
    "  - Currently not working as Workflow names or CronWorkflow names don't match the generated ones by `couler`\n",
    "- Remove its PVC and related PV (created during Argo Workflow execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05912d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.delete_app(app=build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f087cdd",
   "metadata": {},
   "source": [
    "### Remove Scanflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310c7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.clean_environment(app=build_app, scanflow_env=data_eng_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c54fcb",
   "metadata": {},
   "source": [
    "## MLFlow debug cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b15533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     import mlflow\n",
    "\n",
    "#     client = ScanflowTrackerClient(scanflow_tracker_local_uri=env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "#     mlflow.set_tracking_uri(client.get_tracker_uri(True))\n",
    "#     # Retrieve the Application experiment\n",
    "    \n",
    "#     reactive_experiment = mlflow.get_experiment_by_name(app_name)\n",
    "#     experiment_id = reactive_experiment.experiment_id\n",
    "\n",
    "#     # Retrieve filtered experiment runs by run_name, ordered by descending end time --> First entry will be the most recent\n",
    "#     runs_df = mlflow.search_runs([experiment_id], filter_string=f\"run_name='{team_name}'\", order_by=[\"end_time DESC\"])\n",
    "#     run_id = runs_df.loc[[0]]['run_id'][0]\n",
    "#     print(run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
