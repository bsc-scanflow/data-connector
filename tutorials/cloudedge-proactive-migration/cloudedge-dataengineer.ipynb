{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acdbd28-a67a-41b4-b4e6-16e3a0a5626e",
   "metadata": {},
   "source": [
    "# CloudEdge DataEngineer (Inference Stage)\n",
    "\n",
    "****Inference Scenarios****\n",
    "\n",
    "| scenarios | reference app | framework | model/dataset |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| batch-inference-workflow | [scenarios/job-pipeline](https://github.com/peiniliu/inference/tree/dev/vision/classification_and_detection/scenarios/job-pipeline) | tensorflow | resnet/dumy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95d7f3-b42a-4c13-8902-e3ca6f2de632",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Make sure to set these environment variables in your session with the proper values. All of them are mandatory except:\n",
    "- `DOCKER_REGISTRY`: if you plan to push the images to a private registry\n",
    "- `DOCKER_TAG`: if you don't want to leave the default `latest` tag\n",
    "- `DOCKER_REGISTRY_USERNAME`: if your private registry requires authentication\n",
    "- `DOCKER_REGISTRY_PASSWORD`: if your private registry requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af202191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for debug purposes, don't leave them enable in the repository!!!\n",
    "# %env WORKDIR=/root/cloudskin/data-connector\n",
    "# %env KUBECONFIG_PATH=/root/.kube/config\n",
    "# %env REACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-reactive-migration/dataengineer\n",
    "# %env SCANFLOW_SERVER_URI=http://10.0.26.8:32002\n",
    "# %env SCANFLOW_TRACKER_URI=http://10.0.26.8:32002\n",
    "# %env MLFLOW_S3_ENDPOINT_URL=http://10.0.26.8:32000\n",
    "# # PostgreSQL URI with credentials\n",
    "# %env SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\n",
    "# # MinIO API endpoint, not console!\n",
    "# %env AWS_ACCESS_KEY_ID=admin\n",
    "# %env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "# %env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# # If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "# %env DOCKER_TAG=feat/reactive-migration\n",
    "# %env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "# %env DOCKER_REGISTRY_PASSWORD=fake-password\n",
    "# %env SCANFLOW_APP_NAME=cloudedge-migration-experiment\n",
    "# %env SCANFLOW_TEAM_NAME=dataengineer\n",
    "# # NEARBYONE CONTROLLER VARIABLES\n",
    "# %env NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
    "# %env NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
    "# %env NBY_ENV_NAME=nearbyone.innovationlab\n",
    "# %env NBY_ENV_EMAIL=fake.username@example.com\n",
    "# %env NBY_ENV_PASSWORD=fake-password\n",
    "# # This is to avoid CI pipelines to deploy anything\n",
    "# %env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f31ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WORKDIR=/home/jolivera/Documents/CloudSkin/Scanflow/data-connector\n",
      "env: KUBECONFIG_PATH=/home/jolivera/.kube/config_ncloud_socat\n",
      "env: PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-proactive-migration/dataengineer\n",
      "env: SCANFLOW_SERVER_URI=http://84.88.189.179:32767\n",
      "env: SCANFLOW_TRACKER_URI=http://84.88.189.179:32766\n",
      "env: MLFLOW_S3_ENDPOINT_URL=http://84.88.189.179:32645\n",
      "env: SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience\n",
      "env: AWS_ACCESS_KEY_ID=scanflow\n",
      "env: AWS_SECRET_ACCESS_KEY=scanflow123\n",
      "env: DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "env: DOCKER_TAG=feat/proactive-migration\n",
      "env: DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
      "env: DOCKER_REGISTRY_PASSWORD=ii6c4bvSp58yzhckoyBA\n",
      "env: SCANFLOW_APP_NAME=cloudedge-proactive-migration-experiment\n",
      "env: SCANFLOW_TEAM_NAME=dataengineer\n",
      "env: NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
      "env: NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
      "env: NBY_ENV_NAME=nearbyone.innovationlab\n",
      "env: NBY_ENV_EMAIL=fake.username@example.com\n",
      "env: NBY_ENV_PASSWORD=fake-password\n",
      "env: LOCAL_DEPLOY=1\n"
     ]
    }
   ],
   "source": [
    "# Only for debug purposes, don't leave them enable in the repository!!!\n",
    "%env WORKDIR=/home/jolivera/Documents/CloudSkin/Scanflow/data-connector\n",
    "%env KUBECONFIG_PATH=/home/jolivera/.kube/config_ncloud_socat\n",
    "%env PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-proactive-migration/dataengineer\n",
    "%env SCANFLOW_SERVER_URI=http://84.88.189.179:32767\n",
    "%env SCANFLOW_TRACKER_URI=http://84.88.189.179:32766\n",
    "%env MLFLOW_S3_ENDPOINT_URL=http://84.88.189.179:32645\n",
    "# PostgreSQL URI with credentials\n",
    "%env SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience\n",
    "# MinIO API endpoint, not console!\n",
    "%env AWS_ACCESS_KEY_ID=scanflow\n",
    "%env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "%env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "%env DOCKER_TAG=feat/proactive-migration\n",
    "%env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "%env DOCKER_REGISTRY_PASSWORD=ii6c4bvSp58yzhckoyBA\n",
    "%env SCANFLOW_APP_NAME=cloudedge-proactive-migration-experiment\n",
    "%env SCANFLOW_TEAM_NAME=dataengineer\n",
    "# NEARBYONE CONTROLLER VARIABLES\n",
    "%env NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
    "%env NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
    "%env NBY_ENV_NAME=nearbyone.innovationlab\n",
    "%env NBY_ENV_EMAIL=fake.username@example.com\n",
    "%env NBY_ENV_PASSWORD=fake-password\n",
    "# This is to avoid CI pipelines to deploy anything\n",
    "%env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79daff1",
   "metadata": {},
   "source": [
    "## Pre-run cleanup\n",
    "\n",
    "Make sure that the experiment isn't already running by removing its namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df8cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Make sure \"scanflow\" path is added in available module paths\n",
    "sys.path.insert(0,'../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc6944",
   "metadata": {},
   "source": [
    "Let's define some useful Kubernetes client functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6664810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kubernetes import config\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.stream import stream\n",
    "from scanflow.tools import env\n",
    "import tarfile\n",
    "import os\n",
    "from time import time, sleep\n",
    "import yaml\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def delete_namespace_and_wait(client: client.CoreV1Api = None, namespace:str = None, timeout:int = 300):\n",
    "    \"\"\"\n",
    "    Deletes a namespace and waits until its deletion is fully terminated.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - A Kubernetes API client; locally initialized if not provided\n",
    "    - namespace: str - The name of the namespace to delete\n",
    "    - timeout: int - Time to wait in seconds before giving up (default: 300)\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "    \n",
    "    try:\n",
    "        # Delete the namespace\n",
    "        client.delete_namespace(name=namespace)\n",
    "        # Wait for the namespace to be completely deleted\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Try fetching the namespace, if it's still there\n",
    "                response = client.read_namespace(name=namespace)\n",
    "                print(f\"Namespace '{namespace}' is still being deleted...\")\n",
    "            except ApiException as e:\n",
    "                if e.status == 404:\n",
    "                    # Namespace is deleted, exit loop\n",
    "                    print(f\"Namespace '{namespace}' has been successfully deleted.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    raise\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Namespace '{namespace}' still exists after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for some time before checking again\n",
    "            sleep(5)\n",
    "    \n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to delete namespace '{namespace}': {e}\")\n",
    "\n",
    "\n",
    "def deploy_pod_and_wait_for_completion(client: client.CoreV1Api = None, yaml_file: str = None, namespace: str = \"default\", timeout: int = 300) -> None:\n",
    "    \"\"\"\n",
    "    Deploy a pod from a YAML manifest and wait until it reaches the Completed state.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - Kubernetes API client; locally initialized if not provided\n",
    "    - namespace: str - Kubernetes namespace (default: \"default\")\n",
    "    - timeout: int - Time to wait in seconds before giving up (default: 300)\n",
    "    \"\"\"\n",
    "\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "\n",
    "    # Load the YAML file\n",
    "    if not yaml_file:\n",
    "        print(f\"Missing YAML file! Please make sure to provide a valid YAML file path\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    with open(yaml_file, 'r') as f:\n",
    "        pod_manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Extract pod name from the manifest\n",
    "    pod_name = pod_manifest['metadata']['name']\n",
    "\n",
    "    try:\n",
    "        # Create the pod\n",
    "        print(f\"Creating pod: {pod_name} in namespace: {namespace}\")\n",
    "        client.create_namespaced_pod(namespace=namespace, body=pod_manifest)\n",
    "\n",
    "        # Wait for the pod to reach Completed state\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Get the pod's current status\n",
    "                pod = client.read_namespaced_pod(name=pod_name, namespace=namespace)\n",
    "                pod_phase = pod.status.phase\n",
    "                print(f\"Pod '{pod_name}' is currently in phase: {pod_phase}\")\n",
    "                \n",
    "                if pod_phase == \"Succeeded\":\n",
    "                    print(f\"Pod '{pod_name}' has completed successfully (Succeeded).\")\n",
    "                    break\n",
    "                elif pod_phase == \"Failed\":\n",
    "                    print(f\"Pod '{pod_name}' has failed.\")\n",
    "                    break\n",
    "                \n",
    "            except ApiException as e:\n",
    "                print(f\"Error fetching pod status: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Pod '{pod_name}' is not in Completed state after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for a few seconds before checking again\n",
    "            sleep(5)\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to create pod '{pod_name}': {e}\")\n",
    "\n",
    "\n",
    "def deploy_pod_and_wait(client: client.CoreV1Api = None, yaml_file: str = None, namespace: str = \"default\", timeout: int = 300) -> None:\n",
    "    \"\"\"\n",
    "    Deploys a pod using a YAML manifest and waits until its state is 'Running'.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - Kubernetes API client; locally initialized if not provided\n",
    "    - yaml_file: str - Path to the YAML file containing the pod manifest.\n",
    "    - namespace: str - Kubernetes namespace (default: 'default').\n",
    "    - timeout: int - Time to wait in seconds before timing out (default: 300).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load YAML file\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        pod_manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Extract pod name from manifest\n",
    "    pod_name = pod_manifest['metadata']['name']\n",
    "    \n",
    "    try:\n",
    "        # Create the pod\n",
    "        print(f\"Creating pod: {pod_name} in namespace: {namespace}\")\n",
    "        client.create_namespaced_pod(namespace=namespace, body=pod_manifest)\n",
    "\n",
    "        # Wait for the pod to reach Running state\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Get the pod's current status\n",
    "                pod = client.read_namespaced_pod(name=pod_name, namespace=namespace)\n",
    "                pod_phase = pod.status.phase\n",
    "                print(f\"Pod '{pod_name}' is currently in phase: {pod_phase}\")\n",
    "                \n",
    "                if pod_phase == \"Running\":\n",
    "                    print(f\"Pod '{pod_name}' is now in Running state.\")\n",
    "                    break\n",
    "                elif pod_phase == \"Failed\":\n",
    "                    print(f\"Pod '{pod_name}' has failed to start.\")\n",
    "                    break\n",
    "                \n",
    "            except ApiException as e:\n",
    "                print(f\"Error fetching pod status: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Pod '{pod_name}' is not in Running state after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for a few seconds before checking again\n",
    "            sleep(5)\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to create pod '{pod_name}': {e}\")\n",
    "\n",
    "\n",
    "def check_if_object_exists_and_ready(\n",
    "    client: client.CoreV1Api = None, # Kubernetes API client; locally initialized if not provided\n",
    "    object_type: str = \"namespace\",  # Type of Kubernetes object: 'namespace' or 'persistentVolumeClaim'\n",
    "    name: str = \"default\",  # Name of the Kubernetes object (namespace or PVC)\n",
    "    namespace: str = None  # Namespace where the object is located (only for PVC)\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a Kubernetes object (namespace or persistentVolumeClaim) exists and is ready.\n",
    "\n",
    "    Parameters:\n",
    "    - object_type: str - Type of Kubernetes object ('namespace' or 'persistentVolumeClaim').\n",
    "    - name: str - Name of the Kubernetes object.\n",
    "    - namespace: str - Namespace where the object is located (only relevant for PVCs).\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the object exists and is ready, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Initialize API clients\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "\n",
    "    try:\n",
    "        if object_type == \"namespace\":\n",
    "            # Check if the namespace exists\n",
    "            print(f\"Checking if namespace '{name}' exists...\")\n",
    "            namespace_obj = client.read_namespace(name=name)\n",
    "            if namespace_obj.status.phase == \"Active\":\n",
    "                print(f\"Namespace '{name}' exists and is Active.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Namespace '{name}' is not Active.\")\n",
    "                return False\n",
    "\n",
    "        elif object_type == \"persistentVolumeClaim\":\n",
    "            if namespace is None:\n",
    "                raise ValueError(\"Namespace must be specified for persistentVolumeClaim check.\")\n",
    "\n",
    "            # Check if the PVC exists and is bound\n",
    "            print(f\"Checking if persistentVolumeClaim '{name}' exists in namespace '{namespace}'...\")\n",
    "            pvc_obj = client.read_namespaced_persistent_volume_claim(name=name, namespace=namespace)\n",
    "            if pvc_obj.status.phase == \"Bound\":\n",
    "                print(f\"PersistentVolumeClaim '{name}' is Bound and ready.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"PersistentVolumeClaim '{name}' is not in Bound state.\")\n",
    "                return False\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported object type '{object_type}'. Use 'namespace' or 'persistentVolumeClaim'.\")\n",
    "\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"{object_type.capitalize()} '{name}' not found.\")\n",
    "        else:\n",
    "            print(f\"Error fetching {object_type} status: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def copy_local_path_to_pod(client: client.CoreV1Api, namespace: str, pod_name: str, local_path: pathlib.Path, dest_path: str, exclude_paths: list = []):\n",
    "    \"\"\"\n",
    "    Transfer the content of a local path to the desired pod\n",
    "    \"\"\"\n",
    "    import re, os\n",
    "\n",
    "    # Define the pattern to exclude undesired paths to transfer\n",
    "    pattern = '.*(?:% s)' % '|'.join(exclude_paths)\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    with tarfile.open(fileobj=buf, mode='w:tar') as tar:\n",
    "        tar.add(\n",
    "            local_path,\n",
    "            arcname=pathlib.Path(dest_path).joinpath(local_path.name),\n",
    "            filter=lambda x: None if re.match(pattern, x.name) else x\n",
    "        )\n",
    "    \n",
    "    commands = [buf.getvalue()]\n",
    "\n",
    "    # Copying file\n",
    "    exec_command = ['tar', 'xvf', '-', '-C', '/']\n",
    "    resp = stream(client.connect_get_namespaced_pod_exec, pod_name, namespace,\n",
    "                         command=exec_command,\n",
    "                         stderr=True, stdin=True,\n",
    "                         stdout=True, tty=False,\n",
    "                         _preload_content=False)\n",
    "\n",
    "    while resp.is_open():\n",
    "        resp.update(timeout=1)\n",
    "        if resp.peek_stdout():\n",
    "            print(f\"STDOUT: {resp.read_stdout()}\")\n",
    "        if resp.peek_stderr():\n",
    "            print(f\"STDERR: {resp.read_stderr()}\")\n",
    "        if commands:\n",
    "            c = commands.pop(0)\n",
    "            resp.write_stdin(c)\n",
    "        else:\n",
    "            break\n",
    "    resp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2714c68",
   "metadata": {},
   "source": [
    "Remove the experiment namespace if it exists in the Kubernetes cluster:\n",
    "- Wait for its proper termination before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eab8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolivera/.kube/config_ncloud_socat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize kube config and client\n",
    "# DEBUG: show the kubeconfig path due to GitHub CI issues\n",
    "print(env.get_env(\"KUBECONFIG_PATH\"))\n",
    "try:\n",
    "    config.load_kube_config(config_file=env.get_env(\"KUBECONFIG_PATH\"))\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Something wrong with kubeconfig at {env.get_env('KUBECONFIG_PATH'): {e}}\")\n",
    "\n",
    "kube_client = client.CoreV1Api()\n",
    "\n",
    "# Look for all available namespaces\n",
    "namespaces = kube_client.list_namespace()\n",
    "# Compose the expected namespace that Scanflow creates based on app_name and team_name\n",
    "environment_namespace = f\"scanflow-{env.get_env('SCANFLOW_APP_NAME')}-{env.get_env('SCANFLOW_TEAM_NAME')}\"\n",
    "\n",
    "# Remove the namespace if it exists\n",
    "for namespace in namespaces.items:\n",
    "    if environment_namespace == namespace.metadata.name:\n",
    "        delete_namespace_and_wait(client=kube_client, namespace=environment_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f4682",
   "metadata": {},
   "source": [
    "Remove any experiment's pre-built docker image as they prevent fresh builds if the `repository:tag` is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0b59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "docker_client=docker.DockerClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839fe15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging containers...\n",
      "Purging docker tags starting with registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer...\n"
     ]
    }
   ],
   "source": [
    "# Also remove any pre-built docker image\n",
    "import docker\n",
    "\n",
    "repository_prefix = f\"{env.get_env('DOCKER_REGISTRY')}/{env.get_env('SCANFLOW_APP_NAME')}-{env.get_env('SCANFLOW_TEAM_NAME')}\"\n",
    "\n",
    "docker_client = docker.DockerClient()\n",
    "\n",
    "# - First remove any unused container\n",
    "print(\"Purging containers...\")\n",
    "docker_client.containers.prune()\n",
    "\n",
    "# - Then prune any image that matches the repository_prefix\n",
    "print(f\"Purging docker tags starting with {repository_prefix}...\")\n",
    "for docker_image in docker_client.images.list():\n",
    "    for tag in docker_image.tags:\n",
    "        if tag.startswith(repository_prefix):\n",
    "            docker_client.images.remove(tag)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ad0a5",
   "metadata": {},
   "source": [
    "## ScanflowClient initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca50738-a9da-45ec-97fa-9d1722b71dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanflow.client import ScanflowClient\n",
    "from scanflow.client import ScanflowDeployerClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208ebc0",
   "metadata": {},
   "source": [
    "### Debug: available environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ed2df1-257b-4bc5-a641-2d65a168dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://84.88.189.179:32767\n",
      "http://84.88.189.179:32766\n",
      "http://84.88.189.179:32645\n",
      "scanflow\n",
      "scanflow123\n",
      "registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "feat/proactive-migration\n"
     ]
    }
   ],
   "source": [
    "print(env.get_env(\"SCANFLOW_SERVER_URI\"))\n",
    "print(env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "print(env.get_env(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "print(env.get_env(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(env.get_env(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "print(env.get_env(\"DOCKER_REGISTRY\"))\n",
    "print(env.get_env(\"DOCKER_TAG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e216da1",
   "metadata": {},
   "source": [
    "Initialize the ScanflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeae4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer\n"
     ]
    }
   ],
   "source": [
    "# App folder - Must point to the folder includeing all 'dataengineer' and 'datascience' folders\n",
    "# for cloudedge-reactive-migration, allocated in examples/cloudedge-reactive-migration\n",
    "app_dir = os.path.join(env.get_env('WORKDIR'), env.get_env('PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR'))\n",
    "print(app_dir)\n",
    "app_name = env.get_env(\"SCANFLOW_APP_NAME\")\n",
    "team_name = env.get_env(\"SCANFLOW_TEAM_NAME\")\n",
    "\n",
    "# Initialize the Scanflow Client\n",
    "scanflow_client = ScanflowClient(\n",
    "    #if you defined \"SCANFLOW_SERVER_URI\", you dont need to provide this\n",
    "    registry=env.get_env(\"DOCKER_REGISTRY\"),\n",
    "    verbose=True,\n",
    "    docker_network_mode=\"host\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddf4c1",
   "metadata": {},
   "source": [
    "## Batch-inference-graph for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a7fd2",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfbbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor stages\n",
    "# - Executor 1: Data retrieval from Prometheus\n",
    "# - Executor 2: Data pre-processing + QoS Predictor\n",
    "\n",
    "# Define common variables for the Application stages\n",
    "output_dir = \"/workflow\"\n",
    "csv_root_path = os.path.join(output_dir, f\"{app_name}-{team_name}\")\n",
    "\n",
    "# executor_1 = scanflow_client.ScanflowExecutor(\n",
    "#     name=\"data-retrieval\",\n",
    "#     mainfile=\"data-retrieval.py\",\n",
    "#     dockerfile=\"Dockerfile_data_retrieval_no_buildkit\",\n",
    "#     image_pull_policy=\"IfNotPresent\",\n",
    "#     parameters={\n",
    "#         'app_name': app_name,\n",
    "#         'team_name': team_name,\n",
    "#         'output_path': csv_root_path,\n",
    "#         'promcsv_config': \"/app/data-retrieval/promql_queries.json\" # Config file already included in the Docker image\n",
    "#         #'promcsv_config': \"/workflow/promql_queries.json\" # Config file for debug purposes, manually included in the workflow PVC\n",
    "#     }\n",
    "# )\n",
    "\n",
    "executor_2 = scanflow_client.ScanflowExecutor(\n",
    "    name=\"prediction\",\n",
    "    mainfile=\"run_longExp.py\",\n",
    "    dockerfile=\"Dockerfile_prediction_no_buildkit\",\n",
    "    image_pull_policy=\"IfNotPresent\",\n",
    "    parameters={\n",
    "                        'random_seed': 42,\n",
    "                        'is_training': 0,\n",
    "                        'freq':'t',\n",
    "                        'root_path': '.data-retrieval/data/',\n",
    "                        'data_path': 'df_prediction.csv',\n",
    "                        'model_id': 'PatchMixer',\n",
    "                        'model': 'PatchMixer',\n",
    "                        'data': 'custom',\n",
    "                        'features': 'MS',\n",
    "                        'target': 'PredictionTimeTS',\n",
    "                        'seq_len': 10,\n",
    "                        'pred_len': 3,\n",
    "                        'label_len': 0,\n",
    "                        'enc_in': 10,\n",
    "                        'e_layers': 1,\n",
    "                        'd_model': 256,\n",
    "                        'dropout': 0.2,\n",
    "                        'head_dropout': 0,\n",
    "                        'patch_len': 16,\n",
    "                        'stride': 8,\n",
    "                        'des': 'Exp',\n",
    "                        'train_epochs': 15,\n",
    "                        'patience': 5,\n",
    "                        'loss_flag': 2,\n",
    "                        'use_gpu': False,\n",
    "                        'itr': 0,\n",
    "                        'batch_size': 256,\n",
    "                        'learning_rate': 0.001,\n",
    "                        'mlflow_loader': True,\n",
    "                        'action': 'download',\n",
    "                        'models_to_download':'onehotencoder.pkl,checkpoint.pth'\n",
    "                    }\n",
    ")\n",
    "\n",
    "# Stages dependencies\n",
    "# TODO: define them once other stages have been developed\n",
    "# dependency_1 = scanflow_client.ScanflowDependency(\n",
    "#     dependee='data-retrieval',\n",
    "#     depender='qos-upload'\n",
    "# )\n",
    "\n",
    "# Predictor workflow: batch-inference-reactive-graph\n",
    "# TODO: add missing executors and dependencies\n",
    "workflow_1 = scanflow_client.ScanflowWorkflow(\n",
    "    name=\"batch-inference-proactive-graph\",\n",
    "    nodes=[executor_2],\n",
    "    # edges=[dependency_1],\n",
    "    type=\"batch\",\n",
    "    cron=\"*/5 * * * *\",\n",
    "    output_dir=output_dir,\n",
    "    image_pull_secrets=[\"cloudskin-registry\"] # Required for Workflow templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1117d",
   "metadata": {},
   "source": [
    "### Planner\n",
    "\n",
    "The planner is responsible to trigger sensor's tasks at a given frequency:\n",
    "- `func_name` must match one of the available functions within the `scanflow/agent/template/planner` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077d38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger = scanflow_client.ScanflowAgentSensor_IntervalTrigger(minutes=5)\n",
    "# sensor = scanflow_client.ScanflowAgentSensor(\n",
    "#     name=\"proactive_watch_qos\",\n",
    "#     isCustom=True,\n",
    "#     func_name=\"proactive_watch_qos\",\n",
    "#     trigger=trigger,\n",
    "#     kwargs={\n",
    "#         'frequency': 300,\n",
    "#         'app_name': env.get_env(\"NBY_SERVICE_NAME\"),\n",
    "#         'nearbyone_env_name': env.get_env(\"NBY_ENV_NAME\"),\n",
    "#         'nearbyone_organization_id': env.get_env(\"NBY_ORGANIZATION_ID\"),\n",
    "#         'nearbyone_env_email': env.get_env(\"NBY_ENV_EMAIL\"),\n",
    "#         'nearbyone_env_password': env.get_env(\"NBY_ENV_PASSWORD\")\n",
    "#     }\n",
    "# )\n",
    "# planner = scanflow_client.ScanflowAgent(\n",
    "#     name=\"planner\",\n",
    "#     # TODO: if the Agent requires additional python dependencies, create a MR to modify the __dockerfile_template_agent(self, agent) function\n",
    "#     # in dockerBuilder.py so it includes a new `requirements` parameter as in Executor templates\n",
    "#     # In the meantime, let the ScanflowClient generate it; else it won't update the ENV sensors with the new `kwargs` keys\n",
    "#     #dockerfile=\"Dockerfile_scanflow_agent\",\n",
    "#     template=\"planner\",\n",
    "#     sensors=[sensor],\n",
    "#     requirements=\"requirements.txt\",\n",
    "#     image_pull_secret=\"cloudskin-registry\", # Required when deploying to Kubernetes cluster (created during deployment)\n",
    "#     image_pull_policy=\"Always\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83129",
   "metadata": {},
   "source": [
    "### Compose the Scanflow Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fd7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = scanflow_client.ScanflowApplication(\n",
    "    app_name=app_name,\n",
    "    app_dir=app_dir,\n",
    "    team_name=team_name,\n",
    "    workflows=[workflow_1],\n",
    "    # agents=[planner]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ed3b3",
   "metadata": {},
   "source": [
    "### DEBUG: show application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6133549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339ddc6",
   "metadata": {},
   "source": [
    "### Build the Scanflow Application\n",
    "- This step builds the Docker images for all the Scanflow executors and uploads them to the container registry (currently hardcoded in the `scanflow` module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89df89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 09:58:09 -  INFO - Building image registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction\n",
      "05-Dec-24 09:58:09 -  INFO - [+] Image [registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction] not found in repository. Building a new one.\n",
      "05-Dec-24 09:58:09 -  INFO - dockerfile for using /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows/prediction/Dockerfile_prediction_no_buildkit from /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:07:25 -  INFO - [+] Image [prediction] was built successfully. image_tag ['registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction:feat-proactive-migration']\n",
      "05-Dec-24 10:10:12 -  INFO - [+] Image [prediction] was pushed to registry successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the Scanflow Tracker Port (32767)\n",
    "build_app = scanflow_client.build_ScanflowApplication(\n",
    "    app=app,\n",
    "    trackerPort=32761, # Change this port to avoid conflict with any svc already using it.\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d71d4",
   "metadata": {},
   "source": [
    "### DEBUG: show built application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6d14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de650749",
   "metadata": {},
   "source": [
    "### Create a ScanflowDeployerClient\n",
    "\n",
    "This client creates the required environment for Scanflow to run the pipelines in a Kubernetes cluster based on the built application. It can:\n",
    "\n",
    "- Create an environment for the Scanflow application within its own namespace\n",
    "- Deploy a local Scanflow Tracker\n",
    "- Run the application as an Argo Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd58a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:10:12 -  INFO - loading kubernetes configuration from /home/jolivera/.kube/config_ncloud_socat\n",
      "05-Dec-24 10:10:12 -  INFO - found local kubernetes configuration\n"
     ]
    }
   ],
   "source": [
    "# Initialize the deployer client\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    deployer_client = ScanflowDeployerClient(\n",
    "        user_type=\"local\",\n",
    "        deployer=\"argo\",\n",
    "        k8s_config_file=env.get_env(\"KUBECONFIG_PATH\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51a4e2",
   "metadata": {},
   "source": [
    "### Deploy the ScanflowEnvironment\n",
    "This creates:\n",
    "- A namespace for the application\n",
    "- A Deployment for the local scanflow tracker\n",
    "- A Deployment for all the agents (in this case there's only the planner)\n",
    "  - Planner doesn't include right now the `scanflow` module, so it must be copied inside the planner's PVC so the container finds it in the `/scanflow/scanflow/scanflow` path\n",
    "\n",
    "Go to your Kubernetes cluster and check that both tracker and planner pods are Running without errors in the `scanflow-cloudedge-reactive-migration-dataengineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffbb638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose a custom ScanflowEnvironment\n",
    "from scanflow.deployer.env import ScanflowEnvironment\n",
    "data_eng_env = ScanflowEnvironment()\n",
    "data_eng_env.namespace=f\"scanflow-{build_app.app_name}-{build_app.team_name}\"\n",
    "# TRACKER STORAGE MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "# - \"scanflow\" db must already exist in postgresql\n",
    "# - \"scanflow\" bucket must already exist in MinIO\n",
    "#data_eng_env.tracker_config.TRACKER_STORAGE = f\"postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\"\n",
    "data_eng_env.tracker_config.TRACKER_STORAGE = env.get_env(\"SCANFLOW_TRACKER_STORAGE\")\n",
    "data_eng_env.tracker_config.TRACKER_ARTIFACT = f\"s3://scanflow/{data_eng_env.namespace}\"\n",
    "# CLIENT CONFIG: REPLACE WITH CURRENTLY DEPLOYED SERVICES IN \"scanflow-server\" namespace\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = env.get_env(\"SCANFLOW_TRACKER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_URI = env.get_env(\"SCANFLOW_TRACKER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_SERVER_URI = env.get_env(\"SCANFLOW_SERVER_URI\")\n",
    "# MINIO MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "data_eng_env.secret.AWS_ACCESS_KEY_ID = env.get_env(\"AWS_ACCESS_KEY_ID\")\n",
    "data_eng_env.secret.AWS_SECRET_ACCESS_KEY = env.get_env(\"AWS_SECRET_ACCESS_KEY\")\n",
    "data_eng_env.secret.MLFLOW_S3_ENDPOINT_URL = env.get_env(\"MLFLOW_S3_ENDPOINT_URL\")\n",
    "data_eng_env.secret.AWS_ENDPOINT_URL = env.get_env(\"AWS_ENDPOINT_URL\")\n",
    "# NEW: configure image pull secret\n",
    "data_eng_env.image_pull_secret.name = \"cloudskin-registry\"\n",
    "data_eng_env.image_pull_secret.registry = env.get_env(\"DOCKER_REGISTRY\")\n",
    "data_eng_env.image_pull_secret.username = env.get_env(\"DOCKER_REGISTRY_USERNAME\")\n",
    "data_eng_env.image_pull_secret.password = env.get_env(\"DOCKER_REGISTRY_PASSWORD\")\n",
    "data_eng_env.image_pull_secret.email = \"cloudskin-project@bsc.es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba826687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:10:12 -  INFO - [++]Creating env\n",
      "05-Dec-24 10:10:12 -  INFO - [++]Creating namespace \"scanflow-cloudedge-proactive-migration-experiment-dataengineer\"\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:10:12 -  INFO - create_namespace true\n",
      "05-Dec-24 10:10:12 -  INFO - [++]Creating Role for 'default service account'\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:12 -  INFO - create_rolebinding info\n",
      "05-Dec-24 10:10:12 -  INFO - [++]Creating s3 secret {'AWS_ACCESS_KEY_ID': 'scanflow', 'AWS_SECRET_ACCESS_KEY': 'scanflow123', 'MLFLOW_S3_ENDPOINT_URL': 'http://84.88.189.179:32645', 'AWS_ENDPOINT_URL': None}\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:12 -  INFO - create_secret true\n",
      "05-Dec-24 10:10:12 -  INFO - [++]Creating tracker configmap {'TRACKER_STORAGE': 'postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience', 'TRACKER_ARTIFACT': 's3://scanflow/scanflow-cloudedge-proactive-migration-experiment-dataengineer'}\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:12 -  INFO - create_configmap true\n",
      "05-Dec-24 10:10:12 -  INFO - [++]Creating client configmap {'SCANFLOW_TRACKER_URI': 'http://84.88.189.179:32766', 'SCANFLOW_SERVER_URI': 'http://84.88.189.179:32767', 'SCANFLOW_TRACKER_LOCAL_URI': 'http://84.88.189.179:32766'}\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:13 -  INFO - create_configmap true\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:13 -  INFO - create_pvc true\n",
      "05-Dec-24 10:10:13 -  INFO - [++]Creating Image Pull Secret for registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:13 -  INFO - create_secret true\n",
      "05-Dec-24 10:10:13 -  INFO - [+] Starting local tracker: [scanflow-tracker].\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:13 -  INFO - create_deployment true \n",
      "05-Dec-24 10:10:13 -  INFO - [+] Created tracker Deployment True\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:13 -  INFO - create_service true\n",
      "05-Dec-24 10:10:13 -  INFO - [+] Created tracker Service True\n"
     ]
    }
   ],
   "source": [
    "# Create the application environment\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.create_environment(\n",
    "        app=build_app,\n",
    "        scanflowEnv=data_eng_env\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e79f1",
   "metadata": {},
   "source": [
    "### Manual task: copy `scanflow` module\n",
    "This step copies this repository version of `scanflow` module inside the environment's PersistentVolumeClaim. The environment creation is done with asynchronous API calls, so we must ensure that both the `namespace` and the `persistentVolumeClaim` are already available before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8719184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if persistentVolumeClaim 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer' exists in namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer'...\n",
      "PersistentVolumeClaim 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer' is Bound and ready.\n",
      "Creating pod: cloudedge-debug-pod in namespace: scanflow-cloudedge-proactive-migration-experiment-dataengineer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pod 'cloudedge-debug-pod' is currently in phase: Pending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pod 'cloudedge-debug-pod' is currently in phase: Pending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pod 'cloudedge-debug-pod' is currently in phase: Running\n",
      "Pod 'cloudedge-debug-pod' is now in Running state.\n"
     ]
    }
   ],
   "source": [
    "# Steps:\n",
    "# - Local variables:\n",
    "debug_pod_yaml = os.path.join(env.get_env(\"WORKDIR\"), \"tutorials\", \"cloudedge-proactive-migration\", \"debug_pod_dataengineer.yaml\")\n",
    "persistent_volume_claim = f\"scanflow-{environment_namespace}\"\n",
    "scanflow_folder = pathlib.Path(os.path.join(env.get_env(\"WORKDIR\"), \"scanflow\"))\n",
    "\n",
    "# - Check that the persistentVolumeClaim is properly Bound\n",
    "while not check_if_object_exists_and_ready(\n",
    "    client=kube_client,\n",
    "    object_type=\"persistentVolumeClaim\",\n",
    "    name=persistent_volume_claim,\n",
    "    namespace=environment_namespace\n",
    "):\n",
    "    # Wait 2 seconds for the next check\n",
    "    sleep(2)\n",
    "\n",
    "# - Deploy a Pod in the environment namespace that mounts the environment's persistentVolumeClaim.\n",
    "#   For now we'll provide a YAML file with the expected name of the PVC, but in the future\n",
    "#   this should be provided either by the ScanflowDeployClient or a Kubernetes API call\n",
    "deploy_pod_and_wait(\n",
    "    client=kube_client,\n",
    "    yaml_file=debug_pod_yaml,\n",
    "    namespace=environment_namespace\n",
    ")\n",
    "\n",
    "# - Once the pod is Running, proceed to compress the `scanflow` folder onto a tar file; then send it to the Pod\n",
    "#   and uncompress it at the destination path\n",
    "copy_local_path_to_pod(\n",
    "    client=kube_client,\n",
    "    namespace=environment_namespace,\n",
    "    pod_name=\"cloudedge-debug-pod\",\n",
    "    local_path=scanflow_folder,\n",
    "    dest_path=\"/scanflow/scanflow\",\n",
    "    exclude_paths=[\"__pycache__\"]\n",
    ")\n",
    "\n",
    "# - We can leave the Pod running for debugging purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59425f90",
   "metadata": {},
   "source": [
    "## Run Workflow to test\n",
    "This composes an Argo CronWorkflow for the application and submits it to the Argo Workflows engine:\n",
    "- Pre-requisites: Argo Workflows must be set to use the `default` service account when no `serviceAccount` is provided in the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "677efde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:10:26 -  INFO - [++] Running workflow: [batch-inference-proactive-graph].\n",
      "05-Dec-24 10:10:26 -  INFO - [+] output dir /workflow\n",
      "05-Dec-24 10:10:26 -  INFO - [+] Create batch-inference-proactive-graph output PVC\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:26 -  INFO - create_pvc true\n",
      "05-Dec-24 10:10:26 -  INFO - output dir created\n",
      "05-Dec-24 10:10:26 -  INFO - env for executor {'AWS_ACCESS_KEY_ID': 'scanflow', 'AWS_SECRET_ACCESS_KEY': 'scanflow123', 'MLFLOW_S3_ENDPOINT_URL': 'http://84.88.189.179:32645', 'AWS_ENDPOINT_URL': None, 'SCANFLOW_TRACKER_URI': 'http://84.88.189.179:32766', 'SCANFLOW_SERVER_URI': 'http://84.88.189.179:32767', 'SCANFLOW_TRACKER_LOCAL_URI': 'http://84.88.189.179:32766'}\n",
      "05-Dec-24 10:10:26 -  INFO - [+] Building workflow: [batch-inference-proactive-graph:prediction].\n",
      "05-Dec-24 10:10:26 -  INFO -  parameters: ['--random_seed', '42', '--is_training', '0', '--freq', 't', '--root_path', '.data-retrieval/data/', '--data_path', 'df_prediction.csv', '--model_id', 'PatchMixer', '--model', 'PatchMixer', '--data', 'custom', '--features', 'MS', '--target', 'PredictionTimeTS', '--seq_len', '10', '--pred_len', '3', '--label_len', '0', '--enc_in', '10', '--e_layers', '1', '--d_model', '256', '--dropout', '0.2', '--head_dropout', '0', '--patch_len', '16', '--stride', '8', '--des', 'Exp', '--train_epochs', '15', '--patience', '5', '--loss_flag', '2', '--use_gpu', 'False', '--itr', '0', '--batch_size', '256', '--learning_rate', '0.001', '--mlflow_loader', 'True', '--action', 'download', '--models_to_download', 'onehotencoder.pkl,checkpoint.pth']\n",
      "05-Dec-24 10:10:26 -  INFO -  command: ['python', '/app/prediction/run_longExp.py']\n",
      "05-Dec-24 10:10:26 -  INFO -  argo executor: IfNotPresent\n",
      "05-Dec-24 10:10:26 -  INFO - [+] Building workflow: [batch-inference-proactive-graph- edges]\n",
      "05-Dec-24 10:10:26 -  INFO - [+] Building workflow: [batch-inference-proactive-graph- dag]\n",
      "05-Dec-24 10:10:26 -  INFO - Argo submitter namespace: scanflow-cloudedge-proactive-migration-experiment-dataengineer\n",
      "05-Dec-24 10:10:26 -  INFO - Found local kubernetes config. Initialized with kube_config.\n",
      "05-Dec-24 10:10:26 -  INFO - Checking workflow name/generatedName batch-inference-proactive-graph-l0gklp93\n",
      "05-Dec-24 10:10:26 -  INFO - Submitting workflow to Argo\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "05-Dec-24 10:10:27 -  INFO - Workflow batch-inference-proactive-graph-l0gklp93 has been submitted in \"scanflow-cloudedge-proactive-migration-experiment-dataengineer\" namespace!\n",
      "05-Dec-24 10:10:27 -  INFO - [+++] Workflow: [batch-inference-proactive-graph] has been submitted to argo {'apiVersion': 'argoproj.io/v1alpha1', 'kind': 'CronWorkflow', 'metadata': {'creationTimestamp': '2024-12-05T09:09:28Z', 'generation': 1, 'managedFields': [{'apiVersion': 'argoproj.io/v1alpha1', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:spec': {}}, 'manager': 'OpenAPI-Generator', 'operation': 'Update', 'time': '2024-12-05T09:09:28Z'}], 'name': 'batch-inference-proactive-graph-l0gklp93', 'namespace': 'scanflow-cloudedge-proactive-migration-experiment-dataengineer', 'resourceVersion': '141090987', 'uid': '1907a76e-9ae2-420e-a444-d419f16fd191'}, 'spec': {'concurrencyPolicy': 'Allow', 'failedJobsHistoryLimit': 1, 'schedule': '*/5 * * * *', 'startingDeadlineSeconds': 10, 'successfulJobsHistoryLimit': 3, 'suspend': False, 'timezone': 'Europe/Madrid', 'workflowSpec': {'entrypoint': 'batch-inference-proactive-graph-l0gklp93', 'imagePullSecrets': [{'name': 'cloudskin-registry'}], 'templates': [{'dag': {'tasks': [{'name': 'prediction', 'template': 'prediction'}]}, 'name': 'batch-inference-proactive-graph-l0gklp93'}, {'container': {'command': ['python', '/app/prediction/run_longExp.py', '--random_seed', '42', '--is_training', '0', '--freq', 't', '--root_path', '.data-retrieval/data/', '--data_path', 'df_prediction.csv', '--model_id', 'PatchMixer', '--model', 'PatchMixer', '--data', 'custom', '--features', 'MS', '--target', 'PredictionTimeTS', '--seq_len', '10', '--pred_len', '3', '--label_len', '0', '--enc_in', '10', '--e_layers', '1', '--d_model', '256', '--dropout', '0.2', '--head_dropout', '0', '--patch_len', '16', '--stride', '8', '--des', 'Exp', '--train_epochs', '15', '--patience', '5', '--loss_flag', '2', '--use_gpu', 'False', '--itr', '0', '--batch_size', '256', '--learning_rate', '0.001', '--mlflow_loader', 'True', '--action', 'download', '--models_to_download', 'onehotencoder.pkl,checkpoint.pth'], 'env': [{'name': 'AWS_ACCESS_KEY_ID', 'value': 'scanflow'}, {'name': 'AWS_SECRET_ACCESS_KEY', 'value': 'scanflow123'}, {'name': 'MLFLOW_S3_ENDPOINT_URL', 'value': 'http://84.88.189.179:32645'}, {'name': 'AWS_ENDPOINT_URL', 'value': 'None'}, {'name': 'SCANFLOW_TRACKER_URI', 'value': 'http://84.88.189.179:32766'}, {'name': 'SCANFLOW_SERVER_URI', 'value': 'http://84.88.189.179:32767'}, {'name': 'SCANFLOW_TRACKER_LOCAL_URI', 'value': 'http://84.88.189.179:32766'}], 'image': 'registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction:feat-proactive-migration', 'imagePullPolicy': 'IfNotPresent', 'volumeMounts': [{'mountPath': '/workflow', 'name': 'outputpath'}, {'mountPath': '/scanflow', 'name': 'scanflowpath'}]}, 'name': 'prediction'}], 'volumes': [{'name': 'outputpath', 'persistentVolumeClaim': {'claimName': 'batch-inference-proactive-graph'}}, {'name': 'scanflowpath', 'persistentVolumeClaim': {'claimName': 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer'}}]}}}\n",
      "05-Dec-24 10:10:27 -  INFO - [+] Workflow: [batch-inference-proactive-graph] was run successfully.\n"
     ]
    }
   ],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.run_app(app=build_app)\n",
    "    # DEBUG - TODO: if using external config files, automate their copy inside the workflow PVC instead of doing it manually\n",
    "    # - Copy Promcsv config file so it is available within the container in the /workflow/promql_queries.json path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2fe2e",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306527d",
   "metadata": {},
   "source": [
    "### Remove Scanflow application\n",
    "This will delete the target Scanflow application:\n",
    "- Remove its Argo Workflow object\n",
    "  - Currently not working as Workflow names or CronWorkflow names don't match the generated ones by `couler`\n",
    "- Remove its PVC and related PV (created during Argo Workflow execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05912d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.delete_app(app=build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f087cdd",
   "metadata": {},
   "source": [
    "### Remove Scanflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "310c7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.clean_environment(app=build_app, scanflow_env=data_eng_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c54fcb",
   "metadata": {},
   "source": [
    "## MLFlow debug cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35b15533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-Dec-24 10:26:39 -  WARNING - Retrying (JitteredRetry(total=4, connect=4, read=5, redirect=5, status=5)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7111e7678110>, 'Connection to 84.88.189.179 timed out. (connect timeout=120)')': /api/2.0/mlflow/experiments/get-by-name?experiment_name=cloudedge-proactive-migration-experiment\n",
      "05-Dec-24 10:28:43 -  WARNING - Retrying (JitteredRetry(total=3, connect=3, read=5, redirect=5, status=5)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7111d71f6cd0>, 'Connection to 84.88.189.179 timed out. (connect timeout=120)')': /api/2.0/mlflow/experiments/get-by-name?experiment_name=cloudedge-proactive-migration-experiment\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(client\u001b[38;5;241m.\u001b[39mget_tracker_uri(\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Retrieve the Application experiment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m reactive_experiment \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m reactive_experiment\u001b[38;5;241m.\u001b[39mexperiment_id\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(experiment_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/tracking/fluent.py:1616\u001b[0m, in \u001b[0;36mget_experiment_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;124;03m    Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03m        Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/tracking/client.py:1249\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:484\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:519\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 519\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     81\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:365\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    364\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 365\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/utils/rest_utils.py:189\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fetch_auth(host_creds\u001b[38;5;241m.\u001b[39mauth)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost_creds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with timeout exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m To increase the timeout, set the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m to a larger value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mto\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m env_value \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_ALLOW_HTTP_REDIRECTS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    829\u001b[0m     )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    829\u001b[0m     )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    414\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/http/client.py:1303\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1301\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1349\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1056\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    import mlflow\n",
    "    from scanflow.client import ScanflowTrackerClient\n",
    "\n",
    "    client = ScanflowTrackerClient(scanflow_tracker_local_uri=env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "    mlflow.set_tracking_uri(client.get_tracker_uri(True))\n",
    "    # Retrieve the Application experiment\n",
    "    \n",
    "    reactive_experiment = mlflow.get_experiment_by_name(app_name)\n",
    "    experiment_id = reactive_experiment.experiment_id\n",
    "    print(experiment_id)\n",
    "\n",
    "    # Retrieve filtered experiment runs by run_name, ordered by descending end time --> First entry will be the most recent\n",
    "    # runs_df = mlflow.search_runs([experiment_id], filter_string=f\"run_name='{team_name}'\", order_by=[\"end_time DESC\"])\n",
    "    # run_id = runs_df.loc[[0]]['run_id'][0]\n",
    "    # print(run_id)\n",
    "\n",
    "    # Delete experiment\n",
    "    #mlflow.delete_experiment(experiment_id=str(experiment_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
