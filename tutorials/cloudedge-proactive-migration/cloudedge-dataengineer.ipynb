{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acdbd28-a67a-41b4-b4e6-16e3a0a5626e",
   "metadata": {},
   "source": [
    "# CloudEdge DataEngineer (Inference Stage)\n",
    "\n",
    "****Inference Scenarios****\n",
    "\n",
    "| scenarios | reference app | framework | model/dataset |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| batch-inference-workflow | [scenarios/job-pipeline](https://github.com/peiniliu/inference/tree/dev/vision/classification_and_detection/scenarios/job-pipeline) | tensorflow | resnet/dumy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95d7f3-b42a-4c13-8902-e3ca6f2de632",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Make sure to set these environment variables in your session with the proper values. All of them are mandatory except:\n",
    "- `DOCKER_REGISTRY`: if you plan to push the images to a private registry\n",
    "- `DOCKER_TAG`: if you don't want to leave the default `latest` tag\n",
    "- `DOCKER_REGISTRY_USERNAME`: if your private registry requires authentication\n",
    "- `DOCKER_REGISTRY_PASSWORD`: if your private registry requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af202191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for debug purposes, don't leave them enable in the repository!!!\n",
    "# %env WORKDIR=/root/cloudskin/data-connector\n",
    "# %env KUBECONFIG_PATH=/root/.kube/config\n",
    "# %env REACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-reactive-migration/dataengineer\n",
    "# %env SCANFLOW_SERVER_URI=http://10.0.26.8:32002\n",
    "# %env SCANFLOW_TRACKER_URI=http://10.0.26.8:32002\n",
    "# %env MLFLOW_S3_ENDPOINT_URL=http://10.0.26.8:32000\n",
    "# # PostgreSQL URI with credentials\n",
    "# %env SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\n",
    "# # MinIO API endpoint, not console!\n",
    "# %env AWS_ACCESS_KEY_ID=admin\n",
    "# %env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "# %env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# # If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "# %env DOCKER_TAG=feat/reactive-migration\n",
    "# %env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "# %env DOCKER_REGISTRY_PASSWORD=fake-password\n",
    "# %env SCANFLOW_APP_NAME=cloudedge-migration-experiment\n",
    "# %env SCANFLOW_TEAM_NAME=dataengineer\n",
    "# # NEARBYONE CONTROLLER VARIABLES\n",
    "# %env NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
    "# %env NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
    "# %env NBY_ENV_NAME=nearbyone.innovationlab\n",
    "# %env NBY_ENV_EMAIL=fake.username@example.com\n",
    "# %env NBY_ENV_PASSWORD=fake-password\n",
    "# # This is to avoid CI pipelines to deploy anything\n",
    "# %env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f31ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WORKDIR=/home/jolivera/Documents/CloudSkin/Scanflow/data-connector\n",
      "env: KUBECONFIG_PATH=/home/jolivera/.kube/config_ncloud_socat\n",
      "env: PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-proactive-migration/dataengineer\n",
      "env: SCANFLOW_SERVER_URI=http://84.88.189.179:32767\n",
      "env: SCANFLOW_TRACKER_URI=http://84.88.189.179:32766\n",
      "env: MLFLOW_S3_ENDPOINT_URL=http://84.88.189.179:32645\n",
      "env: SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience\n",
      "env: AWS_ACCESS_KEY_ID=scanflow\n",
      "env: AWS_SECRET_ACCESS_KEY=scanflow123\n",
      "env: DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "env: DOCKER_TAG=feat/proactive-migration\n",
      "env: DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
      "env: DOCKER_REGISTRY_PASSWORD=ii6c4bvSp58yzhckoyBA\n",
      "env: SCANFLOW_APP_NAME=cloudedge-proactive-migration-experiment\n",
      "env: SCANFLOW_TEAM_NAME=dataengineer\n",
      "env: NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
      "env: NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
      "env: NBY_ENV_NAME=nearbyone.innovationlab\n",
      "env: NBY_ENV_EMAIL=fake.username@example.com\n",
      "env: NBY_ENV_PASSWORD=fake-password\n",
      "env: LOCAL_DEPLOY=1\n"
     ]
    }
   ],
   "source": [
    "# Only for debug purposes, don't leave them enable in the repository!!!\n",
    "%env WORKDIR=/home/jolivera/Documents/CloudSkin/Scanflow/data-connector\n",
    "%env KUBECONFIG_PATH=/home/jolivera/.kube/config_ncloud_socat\n",
    "%env PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR=examples/cloudedge-proactive-migration/dataengineer\n",
    "%env SCANFLOW_SERVER_URI=http://84.88.189.179:32767\n",
    "%env SCANFLOW_TRACKER_URI=http://84.88.189.179:32766\n",
    "%env MLFLOW_S3_ENDPOINT_URL=http://84.88.189.179:32645\n",
    "# PostgreSQL URI with credentials\n",
    "%env SCANFLOW_TRACKER_STORAGE=postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience\n",
    "# MinIO API endpoint, not console!\n",
    "%env AWS_ACCESS_KEY_ID=scanflow\n",
    "%env AWS_SECRET_ACCESS_KEY=scanflow123\n",
    "%env DOCKER_REGISTRY=registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
    "# If you use invalid characters for a tag, Scanflow will replace them with '-'\n",
    "%env DOCKER_TAG=feat/proactive-migration\n",
    "%env DOCKER_REGISTRY_USERNAME=cloudskin-scanflow-builds\n",
    "%env DOCKER_REGISTRY_PASSWORD=ii6c4bvSp58yzhckoyBA\n",
    "%env SCANFLOW_APP_NAME=cloudedge-proactive-migration-experiment\n",
    "%env SCANFLOW_TEAM_NAME=dataengineer\n",
    "# NEARBYONE CONTROLLER VARIABLES\n",
    "%env NBY_SERVICE_NAME=dlstreamer-pipeline-server\n",
    "%env NBY_ORGANIZATION_ID=abcd-1234-uvxyz-9876\n",
    "%env NBY_ENV_NAME=nearbyone.innovationlab\n",
    "%env NBY_ENV_EMAIL=fake.username@example.com\n",
    "%env NBY_ENV_PASSWORD=fake-password\n",
    "# This is to avoid CI pipelines to deploy anything\n",
    "%env LOCAL_DEPLOY=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79daff1",
   "metadata": {},
   "source": [
    "## Pre-run cleanup\n",
    "\n",
    "Make sure that the experiment isn't already running by removing its namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df8cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Make sure \"scanflow\" path is added in available module paths\n",
    "sys.path.insert(0,'../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc6944",
   "metadata": {},
   "source": [
    "Let's define some useful Kubernetes client functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6664810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kubernetes import config\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.stream import stream\n",
    "from scanflow.tools import env\n",
    "import tarfile\n",
    "import os\n",
    "from time import time, sleep\n",
    "import yaml\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def delete_namespace_and_wait(client: client.CoreV1Api = None, namespace:str = None, timeout:int = 300):\n",
    "    \"\"\"\n",
    "    Deletes a namespace and waits until its deletion is fully terminated.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - A Kubernetes API client; locally initialized if not provided\n",
    "    - namespace: str - The name of the namespace to delete\n",
    "    - timeout: int - Time to wait in seconds before giving up (default: 300)\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "    \n",
    "    try:\n",
    "        # Delete the namespace\n",
    "        client.delete_namespace(name=namespace)\n",
    "        # Wait for the namespace to be completely deleted\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Try fetching the namespace, if it's still there\n",
    "                response = client.read_namespace(name=namespace)\n",
    "                print(f\"Namespace '{namespace}' is still being deleted...\")\n",
    "            except ApiException as e:\n",
    "                if e.status == 404:\n",
    "                    # Namespace is deleted, exit loop\n",
    "                    print(f\"Namespace '{namespace}' has been successfully deleted.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    raise\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Namespace '{namespace}' still exists after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for some time before checking again\n",
    "            sleep(5)\n",
    "    \n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to delete namespace '{namespace}': {e}\")\n",
    "\n",
    "\n",
    "def deploy_pod_and_wait_for_completion(client: client.CoreV1Api = None, yaml_file: str = None, namespace: str = \"default\", timeout: int = 300) -> None:\n",
    "    \"\"\"\n",
    "    Deploy a pod from a YAML manifest and wait until it reaches the Completed state.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - Kubernetes API client; locally initialized if not provided\n",
    "    - namespace: str - Kubernetes namespace (default: \"default\")\n",
    "    - timeout: int - Time to wait in seconds before giving up (default: 300)\n",
    "    \"\"\"\n",
    "\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "\n",
    "    # Load the YAML file\n",
    "    if not yaml_file:\n",
    "        print(f\"Missing YAML file! Please make sure to provide a valid YAML file path\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    with open(yaml_file, 'r') as f:\n",
    "        pod_manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Extract pod name from the manifest\n",
    "    pod_name = pod_manifest['metadata']['name']\n",
    "\n",
    "    try:\n",
    "        # Create the pod\n",
    "        print(f\"Creating pod: {pod_name} in namespace: {namespace}\")\n",
    "        client.create_namespaced_pod(namespace=namespace, body=pod_manifest)\n",
    "\n",
    "        # Wait for the pod to reach Completed state\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Get the pod's current status\n",
    "                pod = client.read_namespaced_pod(name=pod_name, namespace=namespace)\n",
    "                pod_phase = pod.status.phase\n",
    "                print(f\"Pod '{pod_name}' is currently in phase: {pod_phase}\")\n",
    "                \n",
    "                if pod_phase == \"Succeeded\":\n",
    "                    print(f\"Pod '{pod_name}' has completed successfully (Succeeded).\")\n",
    "                    break\n",
    "                elif pod_phase == \"Failed\":\n",
    "                    print(f\"Pod '{pod_name}' has failed.\")\n",
    "                    break\n",
    "                \n",
    "            except ApiException as e:\n",
    "                print(f\"Error fetching pod status: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Pod '{pod_name}' is not in Completed state after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for a few seconds before checking again\n",
    "            sleep(5)\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to create pod '{pod_name}': {e}\")\n",
    "\n",
    "\n",
    "def deploy_pod_and_wait(client: client.CoreV1Api = None, yaml_file: str = None, namespace: str = \"default\", timeout: int = 300) -> None:\n",
    "    \"\"\"\n",
    "    Deploys a pod using a YAML manifest and waits until its state is 'Running'.\n",
    "\n",
    "    Parameters:\n",
    "    - client: client.CoreV1Api - Kubernetes API client; locally initialized if not provided\n",
    "    - yaml_file: str - Path to the YAML file containing the pod manifest.\n",
    "    - namespace: str - Kubernetes namespace (default: 'default').\n",
    "    - timeout: int - Time to wait in seconds before timing out (default: 300).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load YAML file\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        pod_manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Extract pod name from manifest\n",
    "    pod_name = pod_manifest['metadata']['name']\n",
    "    \n",
    "    try:\n",
    "        # Create the pod\n",
    "        print(f\"Creating pod: {pod_name} in namespace: {namespace}\")\n",
    "        client.create_namespaced_pod(namespace=namespace, body=pod_manifest)\n",
    "\n",
    "        # Wait for the pod to reach Running state\n",
    "        start_time = time()\n",
    "        while True:\n",
    "            try:\n",
    "                # Get the pod's current status\n",
    "                pod = client.read_namespaced_pod(name=pod_name, namespace=namespace)\n",
    "                pod_phase = pod.status.phase\n",
    "                print(f\"Pod '{pod_name}' is currently in phase: {pod_phase}\")\n",
    "                \n",
    "                if pod_phase == \"Running\":\n",
    "                    print(f\"Pod '{pod_name}' is now in Running state.\")\n",
    "                    break\n",
    "                elif pod_phase == \"Failed\":\n",
    "                    print(f\"Pod '{pod_name}' has failed to start.\")\n",
    "                    break\n",
    "                \n",
    "            except ApiException as e:\n",
    "                print(f\"Error fetching pod status: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Check if timeout is reached\n",
    "            if time() - start_time > timeout:\n",
    "                print(f\"Timeout reached: Pod '{pod_name}' is not in Running state after {timeout} seconds.\")\n",
    "                break\n",
    "\n",
    "            # Wait for a few seconds before checking again\n",
    "            sleep(5)\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"Failed to create pod '{pod_name}': {e}\")\n",
    "\n",
    "\n",
    "def check_if_object_exists_and_ready(\n",
    "    client: client.CoreV1Api = None, # Kubernetes API client; locally initialized if not provided\n",
    "    object_type: str = \"namespace\",  # Type of Kubernetes object: 'namespace' or 'persistentVolumeClaim'\n",
    "    name: str = \"default\",  # Name of the Kubernetes object (namespace or PVC)\n",
    "    namespace: str = None  # Namespace where the object is located (only for PVC)\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a Kubernetes object (namespace or persistentVolumeClaim) exists and is ready.\n",
    "\n",
    "    Parameters:\n",
    "    - object_type: str - Type of Kubernetes object ('namespace' or 'persistentVolumeClaim').\n",
    "    - name: str - Name of the Kubernetes object.\n",
    "    - namespace: str - Namespace where the object is located (only relevant for PVCs).\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the object exists and is ready, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Initialize API clients\n",
    "    if not client:\n",
    "        client = client.CoreV1Api()\n",
    "\n",
    "    try:\n",
    "        if object_type == \"namespace\":\n",
    "            # Check if the namespace exists\n",
    "            print(f\"Checking if namespace '{name}' exists...\")\n",
    "            namespace_obj = client.read_namespace(name=name)\n",
    "            if namespace_obj.status.phase == \"Active\":\n",
    "                print(f\"Namespace '{name}' exists and is Active.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Namespace '{name}' is not Active.\")\n",
    "                return False\n",
    "\n",
    "        elif object_type == \"persistentVolumeClaim\":\n",
    "            if namespace is None:\n",
    "                raise ValueError(\"Namespace must be specified for persistentVolumeClaim check.\")\n",
    "\n",
    "            # Check if the PVC exists and is bound\n",
    "            print(f\"Checking if persistentVolumeClaim '{name}' exists in namespace '{namespace}'...\")\n",
    "            pvc_obj = client.read_namespaced_persistent_volume_claim(name=name, namespace=namespace)\n",
    "            if pvc_obj.status.phase == \"Bound\":\n",
    "                print(f\"PersistentVolumeClaim '{name}' is Bound and ready.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"PersistentVolumeClaim '{name}' is not in Bound state.\")\n",
    "                return False\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported object type '{object_type}'. Use 'namespace' or 'persistentVolumeClaim'.\")\n",
    "\n",
    "    except ApiException as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"{object_type.capitalize()} '{name}' not found.\")\n",
    "        else:\n",
    "            print(f\"Error fetching {object_type} status: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def copy_local_path_to_pod(client: client.CoreV1Api, namespace: str, pod_name: str, local_path: pathlib.Path, dest_path: str, exclude_paths: list = []):\n",
    "    \"\"\"\n",
    "    Transfer the content of a local path to the desired pod\n",
    "    \"\"\"\n",
    "    import re, os\n",
    "\n",
    "    # Define the pattern to exclude undesired paths to transfer\n",
    "    pattern = '.*(?:% s)' % '|'.join(exclude_paths)\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    with tarfile.open(fileobj=buf, mode='w:tar') as tar:\n",
    "        tar.add(\n",
    "            local_path,\n",
    "            arcname=pathlib.Path(dest_path).joinpath(local_path.name),\n",
    "            filter=lambda x: None if re.match(pattern, x.name) else x\n",
    "        )\n",
    "    \n",
    "    commands = [buf.getvalue()]\n",
    "\n",
    "    # Copying file\n",
    "    exec_command = ['tar', 'xvf', '-', '-C', '/']\n",
    "    resp = stream(client.connect_get_namespaced_pod_exec, pod_name, namespace,\n",
    "                         command=exec_command,\n",
    "                         stderr=True, stdin=True,\n",
    "                         stdout=True, tty=False,\n",
    "                         _preload_content=False)\n",
    "\n",
    "    while resp.is_open():\n",
    "        resp.update(timeout=1)\n",
    "        if resp.peek_stdout():\n",
    "            print(f\"STDOUT: {resp.read_stdout()}\")\n",
    "        if resp.peek_stderr():\n",
    "            print(f\"STDERR: {resp.read_stderr()}\")\n",
    "        if commands:\n",
    "            c = commands.pop(0)\n",
    "            resp.write_stdin(c)\n",
    "        else:\n",
    "            break\n",
    "    resp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2714c68",
   "metadata": {},
   "source": [
    "Remove the experiment namespace if it exists in the Kubernetes cluster:\n",
    "- Wait for its proper termination before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eab8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolivera/.kube/config_ncloud_socat\n",
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' is still being deleted...\n",
      "Namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer' has been successfully deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize kube config and client\n",
    "# DEBUG: show the kubeconfig path due to GitHub CI issues\n",
    "print(env.get_env(\"KUBECONFIG_PATH\"))\n",
    "try:\n",
    "    config.load_kube_config(config_file=env.get_env(\"KUBECONFIG_PATH\"))\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Something wrong with kubeconfig at {env.get_env('KUBECONFIG_PATH'): {e}}\")\n",
    "\n",
    "kube_client = client.CoreV1Api()\n",
    "\n",
    "# Look for all available namespaces\n",
    "namespaces = kube_client.list_namespace()\n",
    "# Compose the expected namespace that Scanflow creates based on app_name and team_name\n",
    "environment_namespace = f\"scanflow-{env.get_env('SCANFLOW_APP_NAME')}-{env.get_env('SCANFLOW_TEAM_NAME')}\"\n",
    "\n",
    "# Remove the namespace if it exists\n",
    "for namespace in namespaces.items:\n",
    "    if environment_namespace == namespace.metadata.name:\n",
    "        delete_namespace_and_wait(client=kube_client, namespace=environment_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f4682",
   "metadata": {},
   "source": [
    "Remove any experiment's pre-built docker image as they prevent fresh builds if the `repository:tag` is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0b59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "docker_client=docker.DockerClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839fe15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging containers...\n",
      "Purging docker tags starting with registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer...\n"
     ]
    }
   ],
   "source": [
    "# Also remove any pre-built docker image\n",
    "import docker\n",
    "\n",
    "repository_prefix = f\"{env.get_env('DOCKER_REGISTRY')}/{env.get_env('SCANFLOW_APP_NAME')}-{env.get_env('SCANFLOW_TEAM_NAME')}\"\n",
    "\n",
    "docker_client = docker.DockerClient()\n",
    "\n",
    "# - First remove any unused container\n",
    "print(\"Purging containers...\")\n",
    "docker_client.containers.prune()\n",
    "\n",
    "# - Then prune any image that matches the repository_prefix\n",
    "print(f\"Purging docker tags starting with {repository_prefix}...\")\n",
    "for docker_image in docker_client.images.list():\n",
    "    for tag in docker_image.tags:\n",
    "        if tag.startswith(repository_prefix):\n",
    "            docker_client.images.remove(tag)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ad0a5",
   "metadata": {},
   "source": [
    "## ScanflowClient initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca50738-a9da-45ec-97fa-9d1722b71dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanflow.client import ScanflowClient\n",
    "from scanflow.client import ScanflowDeployerClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208ebc0",
   "metadata": {},
   "source": [
    "### Debug: available environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ed2df1-257b-4bc5-a641-2d65a168dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://84.88.189.179:32767\n",
      "http://84.88.189.179:32766\n",
      "http://84.88.189.179:32645\n",
      "scanflow\n",
      "scanflow123\n",
      "registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "feat/proactive-migration\n"
     ]
    }
   ],
   "source": [
    "print(env.get_env(\"SCANFLOW_SERVER_URI\"))\n",
    "print(env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "print(env.get_env(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "print(env.get_env(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(env.get_env(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "print(env.get_env(\"DOCKER_REGISTRY\"))\n",
    "print(env.get_env(\"DOCKER_TAG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e216da1",
   "metadata": {},
   "source": [
    "Initialize the ScanflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeae4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer\n"
     ]
    }
   ],
   "source": [
    "# App folder - Must point to the folder includeing all 'dataengineer' and 'datascience' folders\n",
    "# for cloudedge-reactive-migration, allocated in examples/cloudedge-reactive-migration\n",
    "app_dir = os.path.join(env.get_env('WORKDIR'), env.get_env('PROACTIVE_MIGRATION_DATAENGINEER_APP_DIR'))\n",
    "print(app_dir)\n",
    "app_name = env.get_env(\"SCANFLOW_APP_NAME\")\n",
    "team_name = env.get_env(\"SCANFLOW_TEAM_NAME\")\n",
    "\n",
    "# Initialize the Scanflow Client\n",
    "scanflow_client = ScanflowClient(\n",
    "    #if you defined \"SCANFLOW_SERVER_URI\", you dont need to provide this\n",
    "    registry=env.get_env(\"DOCKER_REGISTRY\"),\n",
    "    verbose=True,\n",
    "    docker_network_mode=\"host\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddf4c1",
   "metadata": {},
   "source": [
    "## Batch-inference-graph for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a7fd2",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfbbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor stages\n",
    "# - Executor 1: Data retrieval from Prometheus\n",
    "# - Executor 2: Data pre-processing + QoS Predictor\n",
    "\n",
    "# Define common variables for the Application stages\n",
    "output_dir = \"/workflow\"\n",
    "# csv_root_path = os.path.join(output_dir, f\"{app_name}-{team_name}\")\n",
    "\n",
    "executor_1 = scanflow_client.ScanflowExecutor(\n",
    "    name=\"data-retrieval\",\n",
    "    mainfile=\"data-retrieval.py\",\n",
    "    dockerfile=\"Dockerfile_data_retrieval_no_buildkit\",\n",
    "    image_pull_policy=\"Always\",\n",
    "    parameters={\n",
    "        'experiment_name': \"PatchMixer\",\n",
    "        'team_name': team_name,\n",
    "        'output_path':'/workflow/data'\n",
    "    }\n",
    ")\n",
    "\n",
    "executor_2 = scanflow_client.ScanflowExecutor(\n",
    "    name=\"prediction\",\n",
    "    mainfile=\"run_longExp.py\",\n",
    "    dockerfile=\"Dockerfile_prediction_no_buildkit\",\n",
    "    image_pull_policy=\"Always\",\n",
    "    parameters={\n",
    "                        'random_seed': 42,\n",
    "                        'is_training': 0,\n",
    "                        'freq':'t',\n",
    "                        'root_path': '/workflow/data/',\n",
    "                        'data_path': 'df_prediction.csv',\n",
    "                        'data_iterate':True,\n",
    "                        'model_id': 'PatchMixer',\n",
    "                        'model': 'PatchMixer',\n",
    "                        'data': 'custom',\n",
    "                        'features': 'MS',\n",
    "                        'target': 'PredictionTimeTS',\n",
    "                        'seq_len': 10,\n",
    "                        'pred_len': 3,\n",
    "                        'label_len': 0,\n",
    "                        'enc_in': 11,\n",
    "                        'e_layers': 1,\n",
    "                        'd_model': 256,\n",
    "                        'dropout': 0.2,\n",
    "                        'head_dropout': 0,\n",
    "                        'patch_len': 16,\n",
    "                        'stride': 8,\n",
    "                        'des': 'Exp',\n",
    "                        'train_epochs': 15,\n",
    "                        'patience': 5,\n",
    "                        'loss_flag': 2,\n",
    "                        'use_gpu': False,\n",
    "                        'itr': 0,\n",
    "                        'batch_size': 256,\n",
    "                        'learning_rate': 0.001,\n",
    "                        'mlflow_loader': True,\n",
    "                        'action': 'get_uri',\n",
    "                        'models_to_download':'loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder,loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/checkpoint',\n",
    "                        'app_name':app_name,\n",
    "                        'team_name':team_name,\n",
    "                        'checkpoints':'/workflow/model/'\n",
    "                    }\n",
    ")\n",
    "\n",
    "# Stages dependencies\n",
    "# TODO: define them once other stages have been developed\n",
    "dependency_1 = scanflow_client.ScanflowDependency(\n",
    "    dependee='data-retrieval',\n",
    "    depender='prediction'\n",
    ")\n",
    "\n",
    "# Predictor workflow: batch-inference-reactive-graph\n",
    "# TODO: add missing executors and dependencies\n",
    "workflow_1 = scanflow_client.ScanflowWorkflow(\n",
    "    name=\"batch-inference-proactive-graph\",\n",
    "    nodes=[executor_1,executor_2],\n",
    "    # nodes=[executor_1],\n",
    "    edges=[dependency_1],\n",
    "    type=\"batch\",\n",
    "    cron=\"*/5 * * * *\",\n",
    "    output_dir=output_dir,\n",
    "    image_pull_secrets=[\"cloudskin-registry\"] # Required for Workflow templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1117d",
   "metadata": {},
   "source": [
    "### Planner\n",
    "\n",
    "The planner is responsible to trigger sensor's tasks at a given frequency:\n",
    "- `func_name` must match one of the available functions within the `scanflow/agent/template/planner` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077d38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger = scanflow_client.ScanflowAgentSensor_IntervalTrigger(minutes=5)\n",
    "# sensor = scanflow_client.ScanflowAgentSensor(\n",
    "#     name=\"proactive_watch_qos\",\n",
    "#     isCustom=True,\n",
    "#     func_name=\"proactive_watch_qos\",\n",
    "#     trigger=trigger,\n",
    "#     kwargs={\n",
    "#         'frequency': 300,\n",
    "#         'app_name': env.get_env(\"NBY_SERVICE_NAME\"),\n",
    "#         'nearbyone_env_name': env.get_env(\"NBY_ENV_NAME\"),\n",
    "#         'nearbyone_organization_id': env.get_env(\"NBY_ORGANIZATION_ID\"),\n",
    "#         'nearbyone_env_email': env.get_env(\"NBY_ENV_EMAIL\"),\n",
    "#         'nearbyone_env_password': env.get_env(\"NBY_ENV_PASSWORD\")\n",
    "#     }\n",
    "# )\n",
    "# planner = scanflow_client.ScanflowAgent(\n",
    "#     name=\"planner\",\n",
    "#     # TODO: if the Agent requires additional python dependencies, create a MR to modify the __dockerfile_template_agent(self, agent) function\n",
    "#     # in dockerBuilder.py so it includes a new `requirements` parameter as in Executor templates\n",
    "#     # In the meantime, let the ScanflowClient generate it; else it won't update the ENV sensors with the new `kwargs` keys\n",
    "#     #dockerfile=\"Dockerfile_scanflow_agent\",\n",
    "#     template=\"planner\",\n",
    "#     sensors=[sensor],\n",
    "#     requirements=\"requirements.txt\",\n",
    "#     image_pull_secret=\"cloudskin-registry\", # Required when deploying to Kubernetes cluster (created during deployment)\n",
    "#     image_pull_policy=\"Always\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83129",
   "metadata": {},
   "source": [
    "### Compose the Scanflow Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fd7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = scanflow_client.ScanflowApplication(\n",
    "    app_name=app_name,\n",
    "    app_dir=app_dir,\n",
    "    team_name=team_name,\n",
    "    workflows=[workflow_1],\n",
    "    # agents=[planner]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ed3b3",
   "metadata": {},
   "source": [
    "### DEBUG: show application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6133549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339ddc6",
   "metadata": {},
   "source": [
    "### Build the Scanflow Application\n",
    "- This step builds the Docker images for all the Scanflow executors and uploads them to the container registry (currently hardcoded in the `scanflow` module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89df89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Dec-24 16:11:56 -  INFO - Building image registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-data-retrieval\n",
      "17-Dec-24 16:11:56 -  INFO - [+] Image [registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-data-retrieval] not found in repository. Building a new one.\n",
      "17-Dec-24 16:11:56 -  INFO - dockerfile for using /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows/data-retrieval/Dockerfile_data_retrieval_no_buildkit from /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows\n",
      "17-Dec-24 16:11:56 -  INFO - [+] Image [data-retrieval] was built successfully. image_tag ['registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-data-retrieval:feat-proactive-migration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Dec-24 16:11:57 -  INFO - [+] Image [data-retrieval] was pushed to registry successfully.\n",
      "17-Dec-24 16:11:57 -  INFO - Building image registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction\n",
      "17-Dec-24 16:11:57 -  INFO - [+] Image [registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction] not found in repository. Building a new one.\n",
      "17-Dec-24 16:11:57 -  INFO - dockerfile for using /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows/prediction/Dockerfile_prediction_no_buildkit from /home/jolivera/Documents/CloudSkin/Scanflow/data-connector/examples/cloudedge-proactive-migration/dataengineer/workflows\n",
      "17-Dec-24 16:16:26 -  INFO - [+] Image [prediction] was built successfully. image_tag ['registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction:feat-proactive-migration']\n",
      "17-Dec-24 16:20:29 -  INFO - [+] Image [prediction] was pushed to registry successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the Scanflow Tracker Port (32767)\n",
    "build_app = scanflow_client.build_ScanflowApplication(\n",
    "    app=app,\n",
    "    trackerPort=32761, # Change this port to avoid conflict with any svc already using it.\n",
    "    image_pull_secret=\"cloudskin-registry\" # Required when deploying to Kubernetes (created during deployment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d71d4",
   "metadata": {},
   "source": [
    "### DEBUG: show built application config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6d14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de650749",
   "metadata": {},
   "source": [
    "### Create a ScanflowDeployerClient\n",
    "\n",
    "This client creates the required environment for Scanflow to run the pipelines in a Kubernetes cluster based on the built application. It can:\n",
    "\n",
    "- Create an environment for the Scanflow application within its own namespace\n",
    "- Deploy a local Scanflow Tracker\n",
    "- Run the application as an Argo Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd58a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Dec-24 16:20:29 -  INFO - loading kubernetes configuration from /home/jolivera/.kube/config_ncloud_socat\n",
      "17-Dec-24 16:20:29 -  INFO - found local kubernetes configuration\n"
     ]
    }
   ],
   "source": [
    "# Initialize the deployer client\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    deployer_client = ScanflowDeployerClient(\n",
    "        user_type=\"local\",\n",
    "        deployer=\"argo\",\n",
    "        k8s_config_file=env.get_env(\"KUBECONFIG_PATH\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51a4e2",
   "metadata": {},
   "source": [
    "### Deploy the ScanflowEnvironment\n",
    "This creates:\n",
    "- A namespace for the application\n",
    "- A Deployment for the local scanflow tracker\n",
    "- A Deployment for all the agents (in this case there's only the planner)\n",
    "  - Planner doesn't include right now the `scanflow` module, so it must be copied inside the planner's PVC so the container finds it in the `/scanflow/scanflow/scanflow` path\n",
    "\n",
    "Go to your Kubernetes cluster and check that both tracker and planner pods are Running without errors in the `scanflow-cloudedge-reactive-migration-dataengineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffbb638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose a custom ScanflowEnvironment\n",
    "from scanflow.deployer.env import ScanflowEnvironment\n",
    "data_eng_env = ScanflowEnvironment()\n",
    "data_eng_env.namespace=f\"scanflow-{build_app.app_name}-{build_app.team_name}\"\n",
    "# TRACKER STORAGE MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "# - \"scanflow\" db must already exist in postgresql\n",
    "# - \"scanflow\" bucket must already exist in MinIO\n",
    "#data_eng_env.tracker_config.TRACKER_STORAGE = f\"postgresql://postgres:scanflow123@postgresql.scanflow-server/scanflow\"\n",
    "data_eng_env.tracker_config.TRACKER_STORAGE = env.get_env(\"SCANFLOW_TRACKER_STORAGE\")\n",
    "data_eng_env.tracker_config.TRACKER_ARTIFACT = f\"s3://scanflow/{data_eng_env.namespace}\"\n",
    "# CLIENT CONFIG: REPLACE WITH CURRENTLY DEPLOYED SERVICES IN \"scanflow-server\" namespace\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_LOCAL_URI = env.get_env(\"SCANFLOW_TRACKER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_TRACKER_URI = env.get_env(\"SCANFLOW_TRACKER_URI\")\n",
    "data_eng_env.client_config.SCANFLOW_SERVER_URI = env.get_env(\"SCANFLOW_SERVER_URI\")\n",
    "# MINIO MUST BE ALREADY DEPLOYED IN ITS OWN NAMESPACE (i.e: \"scanflow-server\")\n",
    "data_eng_env.secret.AWS_ACCESS_KEY_ID = env.get_env(\"AWS_ACCESS_KEY_ID\")\n",
    "data_eng_env.secret.AWS_SECRET_ACCESS_KEY = env.get_env(\"AWS_SECRET_ACCESS_KEY\")\n",
    "data_eng_env.secret.MLFLOW_S3_ENDPOINT_URL = env.get_env(\"MLFLOW_S3_ENDPOINT_URL\")\n",
    "data_eng_env.secret.AWS_ENDPOINT_URL = env.get_env(\"AWS_ENDPOINT_URL\")\n",
    "# NEW: configure image pull secret\n",
    "data_eng_env.image_pull_secret.name = \"cloudskin-registry\"\n",
    "data_eng_env.image_pull_secret.registry = env.get_env(\"DOCKER_REGISTRY\")\n",
    "data_eng_env.image_pull_secret.username = env.get_env(\"DOCKER_REGISTRY_USERNAME\")\n",
    "data_eng_env.image_pull_secret.password = env.get_env(\"DOCKER_REGISTRY_PASSWORD\")\n",
    "data_eng_env.image_pull_secret.email = \"cloudskin-project@bsc.es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba826687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Dec-24 16:20:29 -  INFO - [++]Creating env\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating namespace \"scanflow-cloudedge-proactive-migration-experiment-dataengineer\"\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_namespace true\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating Role for 'default service account'\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_rolebinding info\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating s3 secret {'AWS_ACCESS_KEY_ID': 'scanflow', 'AWS_SECRET_ACCESS_KEY': 'scanflow123', 'MLFLOW_S3_ENDPOINT_URL': 'http://84.88.189.179:32645', 'AWS_ENDPOINT_URL': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_secret true\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating tracker configmap {'TRACKER_STORAGE': 'postgresql://postgres:scanflow123@scanflow-postgres.scanflow-server.svc.cluster.local/scanflow-cloudedge-datascience', 'TRACKER_ARTIFACT': 's3://scanflow/scanflow-cloudedge-proactive-migration-experiment-dataengineer'}\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_configmap true\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating client configmap {'SCANFLOW_TRACKER_URI': 'http://84.88.189.179:32766', 'SCANFLOW_SERVER_URI': 'http://84.88.189.179:32767', 'SCANFLOW_TRACKER_LOCAL_URI': 'http://84.88.189.179:32766'}\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_configmap true\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_pvc true\n",
      "17-Dec-24 16:20:29 -  INFO - [++]Creating Image Pull Secret for registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_secret true\n",
      "17-Dec-24 16:20:29 -  INFO - [+] Starting local tracker: [scanflow-tracker].\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:29 -  INFO - create_deployment true \n",
      "17-Dec-24 16:20:29 -  INFO - [+] Created tracker Deployment True\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:30 -  INFO - create_service true\n",
      "17-Dec-24 16:20:30 -  INFO - [+] Created tracker Service True\n"
     ]
    }
   ],
   "source": [
    "# Create the application environment\n",
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.create_environment(\n",
    "        app=build_app,\n",
    "        scanflowEnv=data_eng_env\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e79f1",
   "metadata": {},
   "source": [
    "### Manual task: copy `scanflow` module\n",
    "This step copies this repository version of `scanflow` module inside the environment's PersistentVolumeClaim. The environment creation is done with asynchronous API calls, so we must ensure that both the `namespace` and the `persistentVolumeClaim` are already available before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8719184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if persistentVolumeClaim 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer' exists in namespace 'scanflow-cloudedge-proactive-migration-experiment-dataengineer'...\n",
      "PersistentVolumeClaim 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer' is Bound and ready.\n",
      "Creating pod: cloudedge-debug-pod in namespace: scanflow-cloudedge-proactive-migration-experiment-dataengineer\n",
      "Pod 'cloudedge-debug-pod' is currently in phase: Pending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pod 'cloudedge-debug-pod' is currently in phase: Running\n",
      "Pod 'cloudedge-debug-pod' is now in Running state.\n"
     ]
    }
   ],
   "source": [
    "# Steps:\n",
    "# - Local variables:\n",
    "debug_pod_yaml = os.path.join(env.get_env(\"WORKDIR\"), \"tutorials\", \"cloudedge-proactive-migration\", \"debug_pod_dataengineer.yaml\")\n",
    "persistent_volume_claim = f\"scanflow-{environment_namespace}\"\n",
    "scanflow_folder = pathlib.Path(os.path.join(env.get_env(\"WORKDIR\"), \"scanflow\"))\n",
    "\n",
    "# - Check that the persistentVolumeClaim is properly Bound\n",
    "while not check_if_object_exists_and_ready(\n",
    "    client=kube_client,\n",
    "    object_type=\"persistentVolumeClaim\",\n",
    "    name=persistent_volume_claim,\n",
    "    namespace=environment_namespace\n",
    "):\n",
    "    # Wait 2 seconds for the next check\n",
    "    sleep(2)\n",
    "\n",
    "# - Deploy a Pod in the environment namespace that mounts the environment's persistentVolumeClaim.\n",
    "#   For now we'll provide a YAML file with the expected name of the PVC, but in the future\n",
    "#   this should be provided either by the ScanflowDeployClient or a Kubernetes API call\n",
    "deploy_pod_and_wait(\n",
    "    client=kube_client,\n",
    "    yaml_file=debug_pod_yaml,\n",
    "    namespace=environment_namespace\n",
    ")\n",
    "\n",
    "# - Once the pod is Running, proceed to compress the `scanflow` folder onto a tar file; then send it to the Pod\n",
    "#   and uncompress it at the destination path\n",
    "copy_local_path_to_pod(\n",
    "    client=kube_client,\n",
    "    namespace=environment_namespace,\n",
    "    pod_name=\"cloudedge-debug-pod\",\n",
    "    local_path=scanflow_folder,\n",
    "    dest_path=\"/scanflow/scanflow\",\n",
    "    exclude_paths=[\"__pycache__\"]\n",
    ")\n",
    "\n",
    "# - We can leave the Pod running for debugging purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59425f90",
   "metadata": {},
   "source": [
    "## Run Workflow to test\n",
    "This composes an Argo CronWorkflow for the application and submits it to the Argo Workflows engine:\n",
    "- Pre-requisites: Argo Workflows must be set to use the `default` service account when no `serviceAccount` is provided in the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "677efde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Dec-24 16:20:39 -  INFO - [++] Running workflow: [batch-inference-proactive-graph].\n",
      "17-Dec-24 16:20:39 -  INFO - [+] output dir /workflow\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Create batch-inference-proactive-graph output PVC\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:39 -  INFO - create_pvc true\n",
      "17-Dec-24 16:20:39 -  INFO - output dir created\n",
      "17-Dec-24 16:20:39 -  INFO - env for executor {'AWS_ACCESS_KEY_ID': 'scanflow', 'AWS_SECRET_ACCESS_KEY': 'scanflow123', 'MLFLOW_S3_ENDPOINT_URL': 'http://84.88.189.179:32645', 'AWS_ENDPOINT_URL': None, 'SCANFLOW_TRACKER_URI': 'http://84.88.189.179:32766', 'SCANFLOW_SERVER_URI': 'http://84.88.189.179:32767', 'SCANFLOW_TRACKER_LOCAL_URI': 'http://84.88.189.179:32766'}\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Building workflow: [batch-inference-proactive-graph:data-retrieval].\n",
      "17-Dec-24 16:20:39 -  INFO -  parameters: ['--experiment_name', 'PatchMixer', '--team_name', 'dataengineer', '--output_path', '/workflow/data']\n",
      "17-Dec-24 16:20:39 -  INFO -  command: ['python', '/app/data-retrieval/data-retrieval.py']\n",
      "17-Dec-24 16:20:39 -  INFO -  argo executor: Always\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Building workflow: [batch-inference-proactive-graph:prediction].\n",
      "17-Dec-24 16:20:39 -  INFO -  parameters: ['--random_seed', '42', '--is_training', '0', '--freq', 't', '--root_path', '/workflow/data/', '--data_path', 'df_prediction.csv', '--data_iterate', 'True', '--model_id', 'PatchMixer', '--model', 'PatchMixer', '--data', 'custom', '--features', 'MS', '--target', 'PredictionTimeTS', '--seq_len', '10', '--pred_len', '3', '--label_len', '0', '--enc_in', '11', '--e_layers', '1', '--d_model', '256', '--dropout', '0.2', '--head_dropout', '0', '--patch_len', '16', '--stride', '8', '--des', 'Exp', '--train_epochs', '15', '--patience', '5', '--loss_flag', '2', '--use_gpu', 'False', '--itr', '0', '--batch_size', '256', '--learning_rate', '0.001', '--mlflow_loader', 'True', '--action', 'get_uri', '--models_to_download', 'loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder,loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/checkpoint', '--app_name', 'cloudedge-proactive-migration-experiment', '--team_name', 'dataengineer', '--checkpoints', '/workflow/model/']\n",
      "17-Dec-24 16:20:39 -  INFO -  command: ['python', '/app/prediction/run_longExp.py']\n",
      "17-Dec-24 16:20:39 -  INFO -  argo executor: Always\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Building workflow: [batch-inference-proactive-graph- edges]\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Building workflow: [batch-inference-proactive-graph- dag]\n",
      "17-Dec-24 16:20:39 -  INFO - Argo submitter namespace: scanflow-cloudedge-proactive-migration-experiment-dataengineer\n",
      "17-Dec-24 16:20:39 -  INFO - Found local kubernetes config. Initialized with kube_config.\n",
      "17-Dec-24 16:20:39 -  INFO - Checking workflow name/generatedName batch-inference-proactive-graph-dk9e4tsf\n",
      "17-Dec-24 16:20:39 -  INFO - Submitting workflow to Argo\n",
      "/home/jolivera/miniconda3/envs/dataconnector/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host '84.88.189.179'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "17-Dec-24 16:20:39 -  INFO - Workflow batch-inference-proactive-graph-dk9e4tsf has been submitted in \"scanflow-cloudedge-proactive-migration-experiment-dataengineer\" namespace!\n",
      "17-Dec-24 16:20:39 -  INFO - [+++] Workflow: [batch-inference-proactive-graph] has been submitted to argo {'apiVersion': 'argoproj.io/v1alpha1', 'kind': 'CronWorkflow', 'metadata': {'creationTimestamp': '2024-12-17T15:19:24Z', 'generation': 1, 'managedFields': [{'apiVersion': 'argoproj.io/v1alpha1', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:spec': {}}, 'manager': 'OpenAPI-Generator', 'operation': 'Update', 'time': '2024-12-17T15:19:24Z'}], 'name': 'batch-inference-proactive-graph-dk9e4tsf', 'namespace': 'scanflow-cloudedge-proactive-migration-experiment-dataengineer', 'resourceVersion': '146697176', 'uid': '0e1d4e33-7002-42ee-9eca-b94142ef64ac'}, 'spec': {'concurrencyPolicy': 'Allow', 'failedJobsHistoryLimit': 1, 'schedule': '*/5 * * * *', 'startingDeadlineSeconds': 10, 'successfulJobsHistoryLimit': 3, 'suspend': False, 'timezone': 'Europe/Madrid', 'workflowSpec': {'entrypoint': 'batch-inference-proactive-graph-dk9e4tsf', 'imagePullSecrets': [{'name': 'cloudskin-registry'}], 'templates': [{'dag': {'tasks': [{'name': 'data-retrieval', 'template': 'data-retrieval'}, {'dependencies': ['data-retrieval'], 'name': 'prediction', 'template': 'prediction'}]}, 'name': 'batch-inference-proactive-graph-dk9e4tsf'}, {'container': {'command': ['python', '/app/data-retrieval/data-retrieval.py', '--experiment_name', 'PatchMixer', '--team_name', 'dataengineer', '--output_path', '/workflow/data'], 'env': [{'name': 'AWS_ACCESS_KEY_ID', 'value': 'scanflow'}, {'name': 'AWS_SECRET_ACCESS_KEY', 'value': 'scanflow123'}, {'name': 'MLFLOW_S3_ENDPOINT_URL', 'value': 'http://84.88.189.179:32645'}, {'name': 'AWS_ENDPOINT_URL', 'value': 'None'}, {'name': 'SCANFLOW_TRACKER_URI', 'value': 'http://84.88.189.179:32766'}, {'name': 'SCANFLOW_SERVER_URI', 'value': 'http://84.88.189.179:32767'}, {'name': 'SCANFLOW_TRACKER_LOCAL_URI', 'value': 'http://84.88.189.179:32766'}], 'image': 'registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-data-retrieval:feat-proactive-migration', 'imagePullPolicy': 'Always', 'volumeMounts': [{'mountPath': '/workflow', 'name': 'outputpath'}, {'mountPath': '/scanflow', 'name': 'scanflowpath'}]}, 'name': 'data-retrieval'}, {'container': {'command': ['python', '/app/prediction/run_longExp.py', '--random_seed', '42', '--is_training', '0', '--freq', 't', '--root_path', '/workflow/data/', '--data_path', 'df_prediction.csv', '--data_iterate', 'True', '--model_id', 'PatchMixer', '--model', 'PatchMixer', '--data', 'custom', '--features', 'MS', '--target', 'PredictionTimeTS', '--seq_len', '10', '--pred_len', '3', '--label_len', '0', '--enc_in', '11', '--e_layers', '1', '--d_model', '256', '--dropout', '0.2', '--head_dropout', '0', '--patch_len', '16', '--stride', '8', '--des', 'Exp', '--train_epochs', '15', '--patience', '5', '--loss_flag', '2', '--use_gpu', 'False', '--itr', '0', '--batch_size', '256', '--learning_rate', '0.001', '--mlflow_loader', 'True', '--action', 'get_uri', '--models_to_download', 'loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder,loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/checkpoint', '--app_name', 'cloudedge-proactive-migration-experiment', '--team_name', 'dataengineer', '--checkpoints', '/workflow/model/'], 'env': [{'name': 'AWS_ACCESS_KEY_ID', 'value': 'scanflow'}, {'name': 'AWS_SECRET_ACCESS_KEY', 'value': 'scanflow123'}, {'name': 'MLFLOW_S3_ENDPOINT_URL', 'value': 'http://84.88.189.179:32645'}, {'name': 'AWS_ENDPOINT_URL', 'value': 'None'}, {'name': 'SCANFLOW_TRACKER_URI', 'value': 'http://84.88.189.179:32766'}, {'name': 'SCANFLOW_SERVER_URI', 'value': 'http://84.88.189.179:32767'}, {'name': 'SCANFLOW_TRACKER_LOCAL_URI', 'value': 'http://84.88.189.179:32766'}], 'image': 'registry.gitlab.bsc.es/datacentric-computing/cloudskin-project/cloudskin-registry/cloudedge-proactive-migration-experiment-dataengineer-batch-inference-proactive-graph-prediction:feat-proactive-migration', 'imagePullPolicy': 'Always', 'volumeMounts': [{'mountPath': '/workflow', 'name': 'outputpath'}, {'mountPath': '/scanflow', 'name': 'scanflowpath'}]}, 'name': 'prediction'}], 'volumes': [{'name': 'outputpath', 'persistentVolumeClaim': {'claimName': 'batch-inference-proactive-graph'}}, {'name': 'scanflowpath', 'persistentVolumeClaim': {'claimName': 'scanflow-scanflow-cloudedge-proactive-migration-experiment-dataengineer'}}]}}}\n",
      "17-Dec-24 16:20:39 -  INFO - [+] Workflow: [batch-inference-proactive-graph] was run successfully.\n"
     ]
    }
   ],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    await deployer_client.run_app(app=build_app)\n",
    "    # DEBUG - TODO: if using external config files, automate their copy inside the workflow PVC instead of doing it manually\n",
    "    # - Copy Promcsv config file so it is available within the container in the /workflow/promql_queries.json path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2fe2e",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306527d",
   "metadata": {},
   "source": [
    "### Remove Scanflow application\n",
    "This will delete the target Scanflow application:\n",
    "- Remove its Argo Workflow object\n",
    "  - Currently not working as Workflow names or CronWorkflow names don't match the generated ones by `couler`\n",
    "- Remove its PVC and related PV (created during Argo Workflow execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05912d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.delete_app(app=build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f087cdd",
   "metadata": {},
   "source": [
    "### Remove Scanflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "310c7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "#     await deployer_client.clean_environment(app=build_app, scanflow_env=data_eng_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c54fcb",
   "metadata": {},
   "source": [
    "## MLFlow debug cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba6fc734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SCANFLOW_TRACKER_debug_URI=http://localhost:32766\n"
     ]
    }
   ],
   "source": [
    "#### Debug env variables:\n",
    "%env SCANFLOW_TRACKER_debug_URI=http://localhost:32766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35b15533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "http://localhost:32766\n",
      "s3://scanflow/35/1823222375e842408413709919a2961e/artifacts/loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder\n"
     ]
    }
   ],
   "source": [
    "if env.get_env(\"LOCAL_DEPLOY\"):\n",
    "    import mlflow\n",
    "    from scanflow.client import ScanflowTrackerClient\n",
    "\n",
    "    client = ScanflowTrackerClient(scanflow_tracker_local_uri=env.get_env(\"SCANFLOW_TRACKER_debug_URI\"))\n",
    "    mlflow.set_tracking_uri(client.get_tracker_uri(True))\n",
    "    # Retrieve the Application experiment\n",
    "    \n",
    "    reactive_experiment = mlflow.get_experiment_by_name(\"PatchMixer\")\n",
    "    experiment_id = reactive_experiment.experiment_id\n",
    "    print(experiment_id)\n",
    "\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import mlflow.sklearn\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    from scanflow.tracker.utils import (\n",
    "    get_tracker_uri,\n",
    "    )\n",
    "    print(get_tracker_uri(True))\n",
    "    model_name=\"loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder\"\n",
    "    client = MlflowClient(get_tracker_uri(True))\n",
    "    # client.save_app_model(app_name=app_name,team_name= team_name,model_name=\"onehotencoder\")\n",
    "    mv=client.get_latest_versions(model_name)\n",
    "    print(client.get_model_version_download_uri(model_name,mv[0].version))\n",
    "    # mlflow.sklearn.load_model(model_uri=\"models:/loss_flag2_lr0.001_dm256_PatchMixer_PatchMixer_custom_ftMS_sl10_pl3_p16s8_random42_0/onehotencoder/\")\n",
    "\n",
    "    # Retrieve filtered experiment runs by run_name, ordered by descending end time --> First entry will be the most recent\n",
    "    # runs_df = mlflow.search_runs([experiment_id], filter_string=f\"run_name='{team_name}'\", order_by=[\"end_time DESC\"])\n",
    "    # run_id = runs_df.loc[[0]]['run_id'][0]\n",
    "    # print(run_id)\n",
    "\n",
    "    # Delete experiment\n",
    "    #mlflow.delete_experiment(experiment_id=str(experiment_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
